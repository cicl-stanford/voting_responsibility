---
title: "Responsibility in voting scenarios"
author: "Tobias Gerstenberg & Antonia Langenhoff"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    toc_float: true
    theme: cosmo
    highlight: tango
---

# Load packages 

```{r, message=FALSE}
library("RSQLite")      # for reading in db files 
library("tidyjson")     # for reading in json files 
library("knitr")        # for RMarkdown 
library("lubridate")    # for dealing with dates
library("lsr")          # for wideToLong() function
library("janitor")      # for cleaning variable names 
library("DT")           # nice html tables
library("Hmisc")        # bootstrapped confidence intervals
library("broom.mixed")  # tidy regression results
library("brms")         # Bayesian mixed effects models
library("tidybayes")    # for tidying up results from brms
library("emmeans")      # for estimated marginal means 
library("ggeffects")    # for effects plots 
library("xtable")       # export table to latex
library("kableExtra")   # nice html data tables in Rmarkdown
library("corrr")        # for correlations 
library("patchwork")    # for making figure panels
library("tidyverse")    # for everything else
```

```{r}
# set ggplot theme 
theme_set(theme_classic())

# set knitr options 
opts_chunk$set(comment = "",
               results = "hold",
               fig.show = "hold")

# suppress summarize warnings 
options(dplyr.summarise.inform = F)
```

# Functions 

```{r}
# function for printing out html or latex tables 
print_table = function(data, format = "html", digits = 2){
  if(format == "html"){
    data %>% 
      kable(digits = digits) %>% 
      kable_styling()
  }else if(format == "latex"){
    data %>% 
      xtable(digits = digits,
             caption = "Caption",
             label = "tab:table") %>%
      print(include.rownames = F,
            booktabs = T,
            sanitize.colnames.function = identity,
            caption.placement = "top")
  }
}

# root mean squared error 
rmse = function(x, y){
  return(sqrt(mean((x - y)^2)))
}


# brms table 
fun_brms_table = function(fit, format = "html"){
  fit %>% 
    tidy() %>% 
    filter(effect == "fixed") %>% 
    mutate(term = str_replace(term, fixed("(Intercept)"), "intercept")) %>% 
    select(term:conf.high) %>% 
    rename(`lower 95\\% HDI` = conf.low,
           `upper 95\\% HDI` = conf.high) %>% 
    print_table(format = format)
}

```

# Experiment 1: Importance and surprise judgments 

## Read in and structure data 

### Read in data 

```{r}
df.exp1 = read.csv(file="../../data/experiment_1/experiment_1.csv",
                       stringsAsFactors = F) %>% 
  clean_names() %>% 
  filter(test_i_1 != "") %>% # filter complete participants 
  mutate(across(c(duration_in_seconds, test_i_1:x15_s_1, age_1_text), 
                as.numeric))
```

### Bayesian inference model

```{r bayesian-surprise-model, eval=F}
library("greta")

func_surprise_model = function(data, a, b){
  data = unlist(data)
  
  n_party = c(data["n_same"], data["n_other"])
  n_yes = c(data["n_same_yes"], data["n_other_yes"])

  # prior
  party_same = beta(shape1 = a, shape2 = b)
  party_other = beta(shape1 = a, shape2 = b)
  policy = beta(shape1 = (a + b) / 2, shape2 = (a + b) / 2)

  # average parametrization
  vote_same = (party_same + policy)/2
  vote_other = (party_other + policy)/2

  vote = c(vote_same, vote_other)

  # likelihood
  distribution(n_yes) = binomial(size = n_party, prob = vote)

  # posterior
  model = model(party_same,
                party_other,
                policy,
                vote_same,
                vote_other)

  draws = mcmc(model,
            n_samples = 1000,
            warmup = 1000,
            chains = 4) %>%
    tidy_draws()

  return(draws)
}

# define the cases of interest 
n_same = 0:4
n_other = 0:4
n_same_yes = 0:4
n_other_yes = 0:4

df.cases = crossing(n_same,
                    n_other,
                    n_same_yes,
                    n_other_yes) %>% 
  filter((n_same + n_other == 4) | 
        (n_same + n_other == 2),
         n_same_yes <= n_same,
         n_other_yes <= n_other) %>% 
  mutate(situation = 1:n()) %>% 
  select(situation, everything())

# run the model
df.predictions = df.cases %>% 
  group_by(situation) %>%
  nest() %>%
  mutate(samples = map(.x = data,
                       .f = ~ func_surprise_model(.x, a = 10, b = 5)))

```

### Read in model predictions 

```{r}
# load cases
load(file = "data/cases.RData")

# load predictions
load(file = "data/bayesian_surprise_model_a_10_b_5.RData")

# restructure the predictions
df.predictions = df.predictions %>% 
  unnest(samples) %>% 
  group_by(situation) %>% 
  select(situation, party_other:party_same) %>% 
  summarize_all(mean) %>% 
  left_join(df.cases,
            by = "situation")
```

### Demographic data 

```{r}
df.exp1.demographics = df.exp1 %>% 
  select(response_id, duration_in_seconds, importance:ethnicity) %>% 
  select(-c(age, race)) %>% 
  rename(age = age_1_text,
         race = race_1_text) %>% 
  summarize(time_mean = round(mean(duration_in_seconds/60), 2),
            time_sd = round(sd(duration_in_seconds/60), 2),
            age_mean = round(mean(age)),
            age_sd = round(sd(age)),
            n_female = sum(gender == "Female"))

df.exp1.demographics %>% 
  print_table()
```

### Participant feedback

```{r}
df.exp1 %>% 
  select(importance, surprise) %>% 
  datatable()
```

### Main data 

```{r}
df.exp1.long = df.exp1 %>% 
  select(participant = response_id, x1_i_1:x15_s_1) %>% 
  mutate(participant = 1:nrow(.)) %>% 
  gather("index", "rating", -participant) %>% 
  mutate(index = str_replace_all(index, "_1", "")) %>% 
  separate(index, into = c("trial", "question")) %>% 
  mutate(trial = str_replace_all(trial, "x", ""),
         trial = as.numeric(trial),
         question = factor(question,
                           levels = c("i", "s"),
                           labels = c("importance", "surprise"))) %>% 
  arrange(participant, trial, question)
```


## Predictions 

- read in trialinfo and append model predictions

```{r}
# write predictions for the cases in experiment 1
df.exp1.trialinfo = read.csv(file = "data/experiment1_trialinfo.csv",
                             fileEncoding = "UTF-8-BOM") %>% 
  clean_names() %>% 
  rename(rule = threshold,
         support = supporting_party,
         trial = qualtrics_no) %>% 
  mutate(n_same = rowSums(select(., p1:p5)),
         n_other = 5 - n_same)

df.exp1.predictions = df.exp1.trialinfo %>% 
  select(trial:outcome) %>% 
  gather("index", "value", p1:v5) %>% 
  separate(index, into = c("attribute", "person"), sep = -1) %>% 
  spread(attribute, value) %>% 
  rename(party = p,
         vote = v) %>% 
  group_by(trial) %>% 
  mutate(pivotality = ifelse(vote != outcome, 0, 
                             ifelse(outcome == 0, 
                                    1 / (abs(sum(vote) - rule)),
                                    1 / (abs(sum(vote) - rule) + 1))),
         n_causes = ifelse(vote == 1,
                                 sum(vote),
                                 sum(1-vote))) %>%
  ungroup() %>% 
  mutate(norm_disposition = ifelse(party == vote, 1, 0))
```

```{r}
df.exp1.model = df.exp1.predictions %>% 
  select(trial, focus, person, party, vote) %>% 
  group_by(trial) %>% 
  filter(person != focus) %>% 
  summarize(n_same = sum(party == 1),
            n_other = sum(party == 0),
            n_same_yes = sum(party == 1 & vote == 1),
            n_other_yes = sum(party == 0 & vote == 1)
            ) %>% 
  ungroup() %>% 
  left_join(df.predictions %>% 
              select(-situation),
            by = c("n_same",
                   "n_other",
                   "n_same_yes",
                   "n_other_yes")) %>% 
  left_join(df.exp1.predictions %>%
              filter(focus == person),
            by = "trial") %>% 
  mutate(surprise = NA,
         surprise = ifelse(party == 1 & vote == 1,
                           1 - vote_same,
                           surprise),
         surprise = ifelse(party == 1 & vote == 0,
                           vote_same,
                           surprise),
         surprise = ifelse(party == 0 & vote == 1,
                           1 - vote_other,
                           surprise),
         surprise = ifelse(party == 0 & vote == 0,
                           vote_other,
                           surprise))
```

Table with the list of cases:  

```{r}
df.exp1.trialinfo %>% 
  select(-original_experiment)
```

## Stats

### Bayesian mixed effects models

```{r}
df.exp1.regression = df.exp1.long %>% 
  group_by(trial, question) %>% 
  summarize(rating_mean = mean(rating),
            rating_low = smean.cl.boot(rating)[2],
            rating_high = smean.cl.boot(rating)[3]) %>% 
  ungroup() %>% 
  left_join(df.exp1.model,
            by = "trial")
```

#### Surprise

##### Bayesian surprise model 

```{r}
df.data = df.exp1.long %>% 
  filter(question == "surprise") %>% 
  left_join(df.exp1.model %>% 
              select(trial, surprise),
            by = "trial")

fit_brm_exp1_surprise_bayesian = brm(
  formula = rating ~ 1 + surprise + (1 + surprise | participant),
  data = df.data,
  cores = 2,
  seed = 1,
  control = list(adapt_delta = 0.99),
  file = "cache/fit_brm_exp1_surprise_bayesian_a_10_b_5")
```

##### Dispositional normality 

```{r}
df.data = df.exp1.long %>% 
  filter(question == "surprise") %>% 
  left_join(df.exp1.model %>% 
              select(trial, norm_disposition, norm_situation = n_causes),
            by = "trial")

fit_brm_exp1_surprise_disp = brm(
  formula = rating ~ 1 + norm_disposition + (1 + norm_disposition | participant),
  data = df.data,
  cores = 2,
  seed = 1,
  file = "cache/fit_brm_exp1_surprise_disp")
```

##### Model comparison 

###### r and RMSE

```{r}
func_model_evaluation = function(fit, question_type){
  df.exp1.regression %>% 
    filter(question == question_type) %>% 
    fitted(newdata = .,
           object = fit,
           re_formula = NA) %>% 
    as_tibble() %>% 
    clean_names() %>% 
    bind_cols(df.exp1.regression %>% 
                filter(question == question_type)) %>% 
    summarize(r = cor(estimate, rating_mean),
              rmse = rmse(estimate, rating_mean)) %>% 
    round(2)
}

# bayesian surprise
func_model_evaluation(fit = fit_brm_exp1_surprise_bayesian,
                      question_type = "surprise")

# dispositional
func_model_evaluation(fit = fit_brm_exp1_surprise_disp,
                      question_type = "surprise")
```

###### Model coefficients 

```{r, warning=F, message=F}
tibble(model = c("fit_brm_exp1_surprise_bayesian",
                 "fit_brm_exp1_surprise_disp")) %>% 
  mutate(fit = map(model, get)) %>% 
  mutate(tidy = map(fit, ~ tidy(.,
                                effects = "fixed",
                                conf.method = "HPDinterval"))) %>% 
  unnest(tidy) %>% 
  select(-c(fit, effect, component)) %>% 
  print_table()
```

###### loo

```{r, eval = F}
fit_brm_exp1_surprise_bayesian = add_criterion(fit_brm_exp1_surprise_bayesian,
                                               criterion = c("loo", "waic"),
                                               reloo = T)

fit_brm_exp1_surprise_disp = add_criterion(fit_brm_exp1_surprise_disp,
                                               criterion = c("loo", "waic"),
                                               reloo = T)

loo_compare(fit_brm_exp1_surprise_bayesian,
            fit_brm_exp1_surprise_disp)
```

#### Importance

##### Causal attribution model 

```{r}
df.data = df.exp1.long %>% 
  filter(question == "importance") %>% 
  left_join(df.exp1.model %>% 
              select(trial, pivotality, n_causes),
            by = "trial")

fit_brm_exp1_importance_bayesian = brm(
  formula = rating ~ 1 + pivotality + n_causes + (1 + pivotality + n_causes | participant),
  data = df.data,
  cores = 2,
  seed = 1,
  control = list(adapt_delta = 0.99),
  file = "cache/fit_brm_exp1_importance_bayesian")
```

##### Pivotality only

```{r}
df.data = df.exp1.long %>% 
  filter(question == "importance") %>% 
  left_join(df.exp1.model %>% 
              select(trial, pivotality, n_causes),
            by = "trial")

fit_brm_exp1_importance_pivotality = brm(
  formula = rating ~ 1 + pivotality + (1 + pivotality | participant),
  data = df.data,
  cores = 2,
  seed = 1,
  control = list(adapt_delta = 0.99),
  file = "cache/fit_brm_exp1_importance_pivotality")
```

##### Number of causes only

```{r}
df.data = df.exp1.long %>% 
  filter(question == "importance") %>% 
  left_join(df.exp1.model %>% 
              select(trial, pivotality, n_causes),
            by = "trial")

fit_brm_exp1_importance_contribution = brm(
  formula = rating ~ 1 + n_causes + (1 + n_causes | participant),
  data = df.data,
  cores = 2,
  seed = 1,
  control = list(adapt_delta = 0.99),
  file = "cache/fit_brm_exp1_importance_contribution")
```

##### Model comparison 

###### r and rmse 

```{r}
# causal attribution model 
func_model_evaluation(fit = fit_brm_exp1_importance_bayesian,
                      question_type = "importance")

# pivotality model 
func_model_evaluation(fit = fit_brm_exp1_importance_pivotality,
                      question_type = "importance")
         
# number of causes model 
func_model_evaluation(fit = fit_brm_exp1_importance_contribution,
                      question_type = "importance")
```

###### Model coefficients 

```{r, warning=F, message=F}
  tibble(model = c("fit_brm_exp1_importance_bayesian",
                   "fit_brm_exp1_importance_pivotality",
                   "fit_brm_exp1_importance_contribution")) %>% 
  mutate(fit = map(model, get)) %>% 
  mutate(tidy = map(fit, ~ tidy(.,
                                effects = "fixed",
                                conf.method = "HPDinterval"))) %>% 
  unnest(tidy) %>% 
  select(-c(fit, effect, component)) %>% 
  print_table()
```

###### loo

```{r, warning=F, message=F}
fit_brm_exp1_importance_bayesian = add_criterion(fit_brm_exp1_importance_bayesian,
                                               criterion = c("loo", "waic"),
                                               reloo = T)

fit_brm_exp1_importance_pivotality = add_criterion(fit_brm_exp1_importance_pivotality,
                                               criterion = c("loo", "waic"),
                                               reloo = T)

fit_brm_exp1_importance_contribution = add_criterion(fit_brm_exp1_importance_contribution,
                                               criterion = c("loo", "waic"),
                                               reloo = T)

loo_compare(fit_brm_exp1_importance_bayesian,
            fit_brm_exp1_importance_pivotality)

loo_compare(fit_brm_exp1_importance_bayesian,
            fit_brm_exp1_importance_contribution)
```


### Specific hypotheses

#### Surprise

```{r}
df.exp1.posterior_surprise_fitted = df.exp1.regression %>% 
  select(trial,
         question,
         rating = rating_mean,
         surprise) %>% 
  filter(question == "surprise") %>% 
  add_fitted_draws(newdata = .,
                   model = fit_brm_exp1_surprise_bayesian,
                   re_formula = NA) %>% 
  ungroup() %>% 
  select(trial, .value, .draw) %>% 
  spread(trial, .value)

func_posterior_difference = function(data, trial1, trial2){
  data %>% 
    mutate(difference = .data[[trial1]] - .data[[trial2]]) %>% 
    pull(difference) %>% 
    mean_hdci() %>% 
    mutate(across(where(is.numeric), ~round(., 2))) %>% 
    summarize(difference = str_c("(", y, " [", ymin, ", ", ymax, "])")) %>% 
    print()  
}

func_posterior_difference(data = df.exp1.posterior_surprise_fitted,
                          trial1 = "4",
                          trial2 = "3")
```

#### Importance

```{r}
df.exp1.posterior_importance_fitted = df.exp1.regression %>% 
  select(trial,
         question,
         rating_mean,
         pivotality,
         n_causes) %>% 
  spread(question, rating_mean) %>% 
  add_fitted_draws(newdata = .,
                   model = fit_brm_exp1_importance_bayesian,
                   re_formula = NA) %>% 
  ungroup() %>% 
  select(trial, .value, .draw) %>% 
  spread(trial, .value)


# 3 vs. 2 
func_posterior_difference(data = df.exp1.posterior_importance_fitted,
                          trial1 = "3",
                          trial2 = "2")

# 1 and 4 vs. 3 
df.exp1.posterior_importance_fitted %>% 
  mutate(difference = (`1` + `4`)/2 - `3`) %>% 
  pull(difference) %>% 
  mean_hdci() %>% 
  mutate(across(where(is.numeric), ~round(., 2))) %>% 
  summarize(difference = str_c("(", y, " [", ymin, ", ", ymax, "])")) %>% 
  print()

# 4 vs. 1 
func_posterior_difference(data = df.exp1.posterior_importance_fitted,
                          trial1 = "4",
                          trial2 = "1")

```

## Tables 

### Trial information 

```{r}
df.exp1.trialinfo %>% 
  select(trial, person, p1:p5, v1:v5, threshold = rule, outcome) %>% 
  # xtable() %>%
  # print(include.rownames = FALSE)
  kable() %>% 
  kable_styling(fixed_thead = T,
                bootstrap_options = "striped")
```

### Posterior inferences 

```{r}
parameters = fit_brm_exp1_surprise_bayesian %>% 
  fixef() %>% 
  as_tibble() %>%
  pull(Estimate)

df.predictions %>% 
  rowwise() %>%
  mutate(unusual = sum(n_same - n_same_yes) + sum(n_other_yes)) %>% 
  ungroup() %>% 
  filter((unusual / (n_same + n_other)) <= .5) %>% 
  mutate(trial = 1:n()) %>% 
  select(-unusual) %>%
  mutate(surprise_same_yes = parameters[1] + parameters[2] * (1 - vote_same),
         surprise_other_yes = parameters[1] + parameters[2] * (1 - vote_other)) %>% 
  mutate(across(.cols = contains("n_"),
                .fns = ~ as.character(.))) %>%
  mutate(across(.cols = c(contains("party"),
                  contains("vote"),
                  contains("surprise"),
                  policy),
                .fns = ~ round(., 2))) %>% 
  select(trial = situation,
         n_same, n_same_yes, party_same, vote_same, surprise_same_yes, 
         n_other, n_other_yes, party_other, vote_other, surprise_other_yes,
         policy) %>% 
  # xtable() %>%
  # print(include.rownames = FALSE)
  kable() %>% 
  kable_styling(bootstrap_options = "striped") %>% 
  scroll_box(width = "100%")
```

## Plots

### Generative model 

```{r, eval=F}

func_draw_beta = function(shape1, shape2){
  ggplot(data = tibble(x = c(0, 1)),
         mapping = aes(x = x)) +
    stat_function(fun = "dbeta",
                  args = list(shape1 = shape1,
                              shape2 = shape2),
                  size = 3) +
    scale_x_continuous(breaks = seq(0, 1, 0.25),
                       labels = seq(0, 1, 0.25),
                       limits = c(0, 1)) +
    scale_y_continuous(breaks = 0:4,
                       labels = 0:4,
                       limits = c(0, 4)) +
    theme(text = element_text(size = 36))
  # ggsave(str_c("../../figures/plots/beta_",shape1,"_",shape2, ".pdf"),
  #        width = 8,
  #        height = 6)
}

func_draw_beta_vote = function(data, party){
  ggplot(data = tibble(x = data),
         mapping = aes(x = x)) +
    stat_density(size = 3,
                 bw = 0.05,
                 geom = "line") +
    scale_x_continuous(breaks = seq(0, 1, 0.25),
                       labels = seq(0, 1, 0.25),
                       limits = c(0, 1)) +
    scale_y_continuous(breaks = 0:4,
                       labels = 0:4,
                       limits = c(0, 4)) +
    theme(text = element_text(size = 36))
  
  # ggsave(str_c("../../figures/plots/beta_", party, ".pdf"),
  #          width = 8,
  #          height = 6)
}

# prior distributions 
func_draw_beta(shape1 = 10, shape2 = 5)
func_draw_beta(shape1 = 5, shape2 = 10)
func_draw_beta(shape1 = 7.5, shape2 = 7.5)

# voting distributions 
tmp1 = rbeta(100000, shape1 = 10, shape2 = 5)
tmp2 = rbeta(100000, shape1 = 7.5, shape2 = 7.5)
tmp3 = (tmp1 + tmp2) / 2

tmp4 = rbeta(100000, shape1 = 5, shape2 = 10)
tmp5 = (tmp4 + tmp2) / 2

func_draw_beta_vote(data = tmp3, party = "same")
func_draw_beta_vote(data = tmp5, party = "other")
```


### Means for selection

```{r}

# \u21e6 = left arrow
# \u2713 = check mark 
# \u2717 = cross mark 
# \n = line break

x_labels = 
  c("T=5\nS \u2713\u21e6\nS \u2713\nS \u2713\nO \u2713\nO \u2713",
    "T=1\nS \u2713\u21e6\nS \u2713\nS \u2713\nS \u2713\nS \u2713",
    "T=1\nO \u2713\u21e6\nO \u2713\nO \u2717\nO \u2717\nO \u2717",
    "T=1\nO \u2713\u21e6\nO \u2717\nO \u2717\nO \u2717\nO \u2717")

df.plot = df.exp1.long %>% 
  filter(trial < 5) %>% 
  mutate(question = factor(question, levels = c("surprise", "importance")))

# get predictions for the surprise and importance means 
df.plot.model = fit_brm_exp1_importance_bayesian %>% 
  fitted(newdata = df.exp1.regression %>% 
           filter(question == "importance",
                  trial <= 4) %>% 
           select(rating = rating_mean,
                  pivotality,
                  n_causes),
         re_formula = NA) %>% 
  as_tibble() %>% 
  clean_names() %>% 
  mutate(trial = 1:n()) %>% 
  rename(importance_mean = estimate, 
         importance_low = q2_5,
         importance_high = q97_5) %>% 
  left_join(fit_brm_exp1_surprise_bayesian %>% 
              fitted(newdata = df.exp1.regression %>% 
                       filter(question == "surprise",
                              trial <= 4) %>% 
                       select(rating = rating_mean,
                              surprise),
                     re_formula = NA) %>% 
              as_tibble() %>% 
              clean_names() %>% 
              mutate(trial = 1:n()) %>% 
              rename(surprise_mean = estimate, 
                     surprise_low = q2_5,
                     surprise_high = q97_5),
            by = "trial") %>% 
  select(trial, everything(), -contains("error")) %>% 
  gather(key = "key", value = "value", -trial) %>% 
  separate(key, into = c("question", "index")) %>% 
  spread(index, value) %>% 
  rename(rating = mean) %>% 
  mutate(question = factor(question, levels = c("surprise", "importance")))
  
ggplot(data = df.plot,
       mapping = aes(x = trial,
                     y = rating,
                     group = question,
                     fill = question)) + 
  stat_summary(fun = "mean",
               geom = "bar",
               size = 1,
               # shape = 21, 
               color = "black",
               position = position_dodge(width = 0.9)) + 
  stat_summary(fun.data = "mean_cl_boot",
               geom = "linerange",
               size = 1,
               position = position_dodge(width = 0.9)) + 
  geom_point(data = df.plot.model,
             shape = 21,
             size = 4,
             alpha = 0.8,
             position = position_dodge(width = 0.9),
             show.legend = F) + 
  annotate(geom = "text",
           x = 1:4,
           y = 97,
           label = 1:4,
           hjust = 0.5,
           size = 6) + 
  labs(x = "situation",
       y = "mean rating",
       caption = "T = threshold, S = same party, O = other party, \u21e6 = focus, \u2713 = yes, \u2717 = no") + 
  scale_fill_brewer(palette = "Set1") + 
  scale_x_continuous(breaks = c(1:4)-0.15,
                     labels = x_labels) +
  coord_cartesian(ylim = c(0, 101),
                  xlim = c(0.5, 4.5),
                  expand = F) +
  theme(text = element_text(size = 20),
        axis.text.x = element_text(family = "Arial Unicode MS",
                                   hjust = 0),
        axis.title.x = element_blank(),
        axis.ticks.x = element_blank(),
        legend.title = element_blank(),
        legend.text = element_text(margin = margin(l = 0.2, unit = "cm")),
        plot.caption = element_text(family = "Arial Unicode MS",
                                    hjust = 1,
                                    margin = margin(t = 0.5, unit = "cm"),
                                    size = 12))

# ggsave(file = "../../figures/plots/experiment1_bars.pdf",
#        width = 8,
#        height = 5,
#        device = cairo_pdf)
```


### Scatterplots 

#### Surprise 

```{r fig.height=5, fig.width=5}
df.plot = fit_brm_exp1_surprise_bayesian %>%
  fitted(newdata = df.exp1.regression %>% 
           filter(question == "surprise"),
         re_formula = NA) %>% 
  as_tibble() %>% 
  clean_names() %>%
  bind_cols(df.exp1.regression %>%
              filter(question == "surprise")) %>%
  mutate(color = ifelse(trial <= 4, 1, 0),
         color = as.factor(color)) %>% 
  arrange(desc(trial)) %>% 
  rename(rating = rating_mean)


x = "estimate"
y = "rating"

ggplot(data = df.plot,
       mapping = aes(x = .data[[x]],
                     y = .data[[y]],
                     color = color,
                     label = trial)) +
  geom_abline(intercept = 0,
              slope = 1,
              linetype = 2) +
  geom_smooth(mapping = aes(y = .data[[x]],
                            ymin = q2_5,
                            ymax = q97_5),
              stat = "identity",
              color = "lightblue",
              alpha = 0.4,
              fill = "lightblue") +
  geom_linerange(mapping = aes_string(ymin = str_c(y, "_low"),
                               ymax = str_c(y, "_high")),
                 alpha = 0.2) +
  geom_point(size = 2) +
  geom_text(data = df.plot %>% filter(trial <= 4),
            nudge_x = 4,
            nudge_y = -1,
            size = 6) +
  annotate(geom = "text",
           label = df.plot %>% 
             summarize(r = cor(.data[[x]], .data[[y]]),
                       rmse = rmse(.data[[x]], .data[[y]])) %>% 
             mutate(across(.fns = ~ round(., 2))) %>% 
             unlist() %>% 
             str_c(names(.), " = ", .),
           x = c(0, 0), 
           y = c(100, 90),
           size = 7,
           hjust = 0) + 
  labs(x = "dispositional inference model",
       y = "mean surprise rating") +
  scale_color_manual(values = c("black", "#e41a1c"),
                     guide = F) +
  scale_x_continuous(breaks = seq(0, 100, 25)) +
  scale_y_continuous(breaks = seq(0, 100, 25)) +
  coord_cartesian(xlim = c(0, 100),
                  ylim = c(0, 100)) +
  theme(text = element_text(size = 24),
        axis.title.x = element_text(size = 22))

# ggsave(file = "../../figures/plots/experiment1_surprise_scatter.pdf",
#        width = 5,
#        height = 5)
```


#### Importance 

```{r fig.height=5, fig.width=5}
df.plot = fit_brm_exp1_importance_bayesian %>% 
  fitted(newdata = df.exp1.regression %>% 
           filter(question == "importance"),
         re_formula = NA) %>% 
  as_tibble() %>% 
  clean_names() %>%
  bind_cols(df.exp1.regression %>%
              filter(question == "importance")) %>%
  mutate(color = ifelse(trial <= 4, 1, 0),
         color = as.factor(color)) %>% 
  arrange(desc(trial)) %>% 
  rename(rating = rating_mean)


x = "estimate"
y = "rating"

ggplot(data = df.plot,
       mapping = aes(x = .data[[x]],
                     y = .data[[y]],
                     color = color,
                     label = trial)) +
  geom_abline(intercept = 0,
              slope = 1,
              linetype = 2) +
  geom_smooth(mapping = aes(y = .data[[x]],
                            ymin = q2_5,
                            ymax = q97_5),
              stat = "identity",
              color = "lightblue",
              alpha = 0.4,
              fill = "lightblue") +
  geom_linerange(mapping = aes_string(ymin = str_c(y, "_low"),
                               ymax = str_c(y, "_high")),
                 alpha = 0.2) +
  geom_point(size = 2) +
  geom_text(data = df.plot %>% filter(trial <= 4),
            nudge_x = 4,
            nudge_y = -1,
            size = 6) +
  annotate(geom = "text",
           label = df.plot %>% 
             summarize(r = cor(.data[[x]], .data[[y]]),
                       rmse = rmse(.data[[x]], .data[[y]])) %>% 
             mutate(across(.fns = ~ round(., 2))) %>% 
             unlist() %>% 
             str_c(names(.), " = ", .),
           x = c(0, 0), 
           y = c(100, 90),
           size = 7,
           hjust = 0) + 
  labs(x = "causal attribution model",
       y = "mean importance rating") +
  scale_color_manual(values = c("black", "#377eb8"),
                     guide = F) +
  scale_x_continuous(breaks = seq(0, 100, 25)) +
  scale_y_continuous(breaks = seq(0, 100, 25)) +
  coord_cartesian(xlim = c(0, 100),
                  ylim = c(0, 100)) +
  theme(text = element_text(size = 24),
        axis.title.x = element_text(size = 22))

# ggsave(file = "../../figures/plots/experiment1_importance_scatter.pdf",
#        width = 5,
#        height = 5)
```

### Tile plot of Bayesian model fit 

```{r}
load(file = "data/bayesian_model_fits_surprise.RData")

ggplot(data = df.bayesian_fit,
       mapping = aes(x = a,
                     y = b,
                     fill = rmse)) +
  geom_tile(color = "black") +
  geom_point(data = df.bayesian_fit %>% 
               filter(rmse == min(rmse)),
             shape = 21, 
             fill = "red",
             size =  5) + 
  scale_fill_gradient(high = "white", low = "black") + 
  scale_x_continuous(breaks = 2:19) + 
  scale_y_continuous(breaks = 1:9) +
  theme(text = element_text(size = 20),
        legend.position = c(1, 1),
        legend.justification = c(1, 1))

# ggsave("../../figures/plots/experiment1_tiles.pdf",
#        width = 8, 
#        height = 6)
```


# Experiment 2: Responsibility judgments

## Read in data 

```{r}
con = dbConnect(SQLite(),dbname = "../../data/experiment_2/experiment_2.db");
df.exp2 = dbReadTable(con,"voting_three_five")
dbDisconnect(con)
df.exp2 = df.exp2 %>% 
  filter(status %in% c(3, 4))
```

## Demographics 

```{r}
df.exp2.demographics = df.exp2$datastring %>% 
  as.tbl_json() %>%
  enter_object("questiondata") %>% 
  gather_object() %>% 
  append_values_string() %>% 
  as_tibble() %>% 
  rename(participant = document.id) %>% 
  spread(name, string) %>% 
  mutate(age = as.numeric(age),
         condition = as.factor(condition)) %>% 
  mutate(begin = df.exp2$beginhit,
         end =  df.exp2$endhit,
         time = as.duration(interval(ymd_hms(df.exp2$beginexp), ymd_hms(df.exp2$endhit)))) %>% 
  select(participant, sex, age, condition, time, feedback)
```

```{r}
df.exp2.demographics %>% 
  summarize(age_mean = round(mean(age)),
            age_sd = round(sd(age)),
            time_mean = round(mean(time)/60, 2),
            time_sd = round(sd(time)/60, 2),
            n_female = sum(sex == "female")) %>% 
  print_table()
```

### Participant feedback 

```{r}
df.exp2.demographics %>% 
  select(participant, feedback) %>% 
  datatable()
```


## Responsibility judgments 

```{r}
df.exp2.wide = data.frame(participant = 1:nrow(df.exp2))

for (i in 1:nrow(df.exp2)){
  a = rjson::fromJSON(df.exp2$datastring[i])
  for (j in 1:length(a$data)){
    df.exp2.wide[[str_c("trial_",j-1)]][i] = 
      a$data[[j]]$trialdata[[1]]
    df.exp2.wide[[str_c("rating1_",j-1)]][i] = 
      a$data[[j]]$trialdata[[3]][[1]]
    df.exp2.wide[[str_c("rating2_",j-1)]][i] = 
      a$data[[j]]$trialdata[[3]][[2]]
    df.exp2.wide[[str_c("rating3_",j-1)]][i] = 
      a$data[[j]]$trialdata[[3]][[3]]
    df.exp2.wide[[str_c("rating4_",j-1)]][i] = 
      a$data[[j]]$trialdata[[3]][[4]]
    df.exp2.wide[[str_c("rating5_",j-1)]][i] = 
      a$data[[j]]$trialdata[[3]][[5]]
    df.exp2.wide[[str_c("party1_",j-1)]][i] = 
      a$data[[j]]$trialdata[[5]][1]
    df.exp2.wide[[str_c("party2_",j-1)]][i] = 
      a$data[[j]]$trialdata[[5]][2]
    df.exp2.wide[[str_c("party3_",j-1)]][i] = 
      a$data[[j]]$trialdata[[5]][3]
    df.exp2.wide[[str_c("party4_",j-1)]][i] = 
      a$data[[j]]$trialdata[[5]][4]
    df.exp2.wide[[str_c("party5_",j-1)]][i] = 
      a$data[[j]]$trialdata[[5]][5]
    df.exp2.wide[[str_c("vote1_",j-1)]][i] = 
      a$data[[j]]$trialdata[[7]][1]
    df.exp2.wide[[str_c("vote2_",j-1)]][i] = 
      a$data[[j]]$trialdata[[7]][2]
    df.exp2.wide[[str_c("vote3_",j-1)]][i] = 
      a$data[[j]]$trialdata[[7]][3]
    df.exp2.wide[[str_c("vote4_",j-1)]][i] = 
      a$data[[j]]$trialdata[[7]][4]
    df.exp2.wide[[str_c("vote5_",j-1)]][i] = 
      a$data[[j]]$trialdata[[7]][5]
    df.exp2.wide[[str_c("support_",j-1)]][i] = 
      a$data[[j]]$trialdata[[9]]
    df.exp2.wide[[str_c("rule_",j-1)]][i] = 
      a$data[[j]]$trialdata[[11]]
    df.exp2.wide[[str_c("outcome_",j-1)]][i] = 
      a$data[[j]]$trialdata[[13]]
  }
}

# make a tidy data frame 
df.exp2.long = wideToLong(df.exp2.wide,
                          within = "trialOrder") %>% 
  select(participant, trial, contains("rating"), contains("party"), contains("vote"), support, rule, outcome) %>% 
  gather("index", "value", rating1:vote5) %>% 
  separate(index, sep = -1, into = c("index", "person")) %>% 
  spread(index, value) %>% 
  mutate(trial = str_remove(trial, "trial_"),
         trial = as.numeric(trial)) %>% 
  group_by(participant) %>% 
  mutate(order = rep(1:17, each = 5),
         rating = ifelse(rating == "NA", NA, rating),
         rating = as.numeric(rating)) %>% 
  ungroup() %>% 
  group_by(participant, trial) %>% 
  mutate(size = ifelse(any(is.na(party)), 3, 5),
         sliders = ifelse(sum(!is.na(rating)) == 2, 2, 1)) %>%
  ungroup() %>% 
  left_join(df.exp2.demographics %>% 
              select(participant, condition),
            by = "participant") %>% 
  mutate(across(c(person, party, vote), as.numeric)) %>% 
  select(participant, condition, trial, order, support, rule, outcome, size, sliders, person, party, vote, rating) %>% 
  filter(!is.na(party))
  
# remove variables 
rm(list = c("df.exp2.wide", "a"))
```

## Model predictions 

Determine predictions for all the cases: 

```{r}
df.exp2.predictions = df.exp2.long %>% 
  select(condition, trial, support, rule, outcome, size, sliders, person, party, vote) %>% 
  distinct() %>% 
  arrange(trial, person) %>% 
  mutate(outcome = ifelse(outcome == "not passed", 0, 1)) %>% 
  na.omit() %>% 
  group_by(trial) %>% 
  mutate(pivotality = ifelse(vote != outcome, 0, 
                             ifelse(outcome == 0, 
                                    1 / (abs(sum(vote) - rule)),
                                    1 / (abs(sum(vote) - rule) + 1))),
         n_causes = ifelse(vote == 1,
                                 sum(vote),
                                 sum(1 - vote))) %>% 
  ungroup() %>% 
  mutate(norm_disposition = ifelse(party == vote, 1, 0))
  
df.exp2.predictions = df.exp2.predictions %>% 
  select(trial:vote) %>% 
  group_by(trial) %>% 
  mutate(n_same = sum(party == 1),
         n_other = sum(party == 0),
         n_same_yes = sum(party == 1 & vote == 1),
         n_other_yes = sum(party == 0 & vote == 1)) %>% 
  ungroup() %>% 
  left_join(df.exp2.long %>% 
              mutate(focus = !is.na(rating),
                     focus = focus * person) %>% 
              select(trial, person, focus) %>% 
              distinct(),
            by = c("trial", "person")) %>% 
  filter(focus != 0) %>% 
  mutate(n_same = ifelse(party == 1,
                         n_same - 1,
                         n_same),
         n_other = ifelse(party == 0,
                         n_other - 1,
                         n_other),
         n_same_yes = ifelse(party == 1 & vote == 1,
                             n_same_yes - 1,
                             n_same_yes),
         n_other_yes = ifelse(party == 0 & vote == 1,
                             n_other_yes - 1,
                             n_other_yes)) %>% 
  group_by(trial) %>% 
  mutate(index = 1:n()) %>% 
  ungroup() %>% 
  left_join(df.exp2.predictions,
            by = c("trial", 
                   "support", 
                   "rule", 
                   "outcome", 
                   "size", 
                   "sliders", 
                   "person", 
                   "party", 
                   "vote")) %>% 
  left_join(df.predictions %>% 
              select(vote_same, vote_other, n_same:n_other_yes),
            by = c("n_same", "n_other", "n_same_yes", "n_other_yes")) %>% 
  mutate(surprise = NA,
         surprise = ifelse(party == 1 & vote == 1,
                           1 - vote_same,
                           surprise),
         surprise = ifelse(party == 1 & vote == 0,
                           vote_same,
                           surprise),
         surprise = ifelse(party == 0 & vote == 1,
                           1 - vote_other,
                           surprise),
         surprise = ifelse(party == 0 & vote == 0,
                           vote_other,
                           surprise)) %>% 
  select(trial, index, condition, everything())
```

## Stats

### Bayesian mixed effects models 

#### For the selection of 24 cases 

##### Structure the data 

```{r}
df.exp2.selection = df.exp2.long %>% 
  mutate(outcome = ifelse(outcome == "not passed", 0 , 1)) %>% 
  left_join(df.exp1.trialinfo %>% 
              select(trial_exp1 = trial, trial = trial_exp2, person),
            by = c("trial", "person")) %>% 
  na.omit() %>% 
  # model predictions 
  left_join(df.exp2.predictions,
            by = c("condition",
                   "trial",
                   "support",
                   "rule",
                   "outcome",
                   "size",
                   "sliders",
                   "person",
                   "party",
                   "vote")) %>% 
  # surprise and importance judgments from experiment 1
  left_join(df.exp1.regression %>% 
              select(trial, question, rating_mean) %>% 
              spread(question, rating_mean) %>% 
              rename(importance_empirical = importance,
                     surprise_empirical = surprise),
            by = c("trial_exp1" = "trial")) %>% 
  na.omit()
```


##### Empirical

```{r}
fit_brm_exp2_selection_empirical = brm(formula = rating ~ 1 + surprise_empirical + importance_empirical + (1 + surprise_empirical + importance_empirical | participant),
                                       data = df.exp2.selection,
                                       cores = 2,
                                       seed = 1,
                                       file = "cache/fit_brm_exp2_selection_empirical")

fit_brm_exp2_selection_empirical %>% summary()
```

##### Model

```{r}
df.data = fitted(newdata = df.exp2.selection,
                 object = fit_brm_exp1_surprise_bayesian,
                 re_formula = NA) %>% 
  as_tibble() %>% 
  clean_names() %>% 
  select(surprise_model = estimate) %>% 
  bind_cols(
    fitted(newdata = df.exp2.selection,
           object = fit_brm_exp1_importance_bayesian,
           re_formula = NA) %>% 
      as_tibble() %>% 
      clean_names() %>% 
      select(importance_model = estimate)
  ) %>% 
  bind_cols(df.exp2.selection)
  
fit_brm_exp2_selection_model = brm(formula = rating ~ 1 + surprise_model + importance_model + (1 + surprise_model + importance_model | participant),
                                       data = df.data,
                                       cores = 2,
                                       seed = 1,
                                       file = "cache/fit_brm_exp2_selection_model")

fit_brm_exp2_selection_model %>% summary()
```

##### Lesioned models 

Surprise only 

```{r}
fit_brm_exp2_selection_empirical_surprise = brm(formula = rating ~ 1 + surprise_empirical + (1 + surprise_empirical | participant),
                                       data = df.exp2.selection,
                                       cores = 2,
                                       seed = 1,
                                       file = "cache/fit_brm_exp2_selection_empirical_surprise")

fit_brm_exp2_selection_empirical_surprise %>% summary()
```

Importance only

```{r}
fit_brm_exp2_selection_empirical_importance = brm(formula = rating ~ 1 + importance_empirical + (1 + importance_empirical | participant),
                                       data = df.exp2.selection,
                                       cores = 2,
                                       seed = 1,
                                       file = "cache/fit_brm_exp2_selection_empirical_importance")

fit_brm_exp2_selection_empirical_importance %>% summary()
```

#### Overall

##### Structure the data 

```{r}
# individual data points
df.exp2.regression = df.exp2.long %>% 
  na.omit() %>% 
  group_by(participant, trial) %>% 
  mutate(index = 1:n()) %>% 
  ungroup() %>% 
  select(-outcome) %>% 
  left_join(df.exp2.predictions %>% 
              select(trial, index, pivotality, n_causes, surprise, outcome, norm_disposition),
            by = c("trial", "index"))

# means 
df.exp2.regression.means = df.exp2.regression %>% 
  group_by(trial, index, support, rule, size, sliders, person, party, vote, pivotality, n_causes, 
           surprise, outcome, norm_disposition) %>% 
  summarize(rating_mean = smean.cl.boot(rating)[1],
            rating_low = smean.cl.boot(rating)[2],
            rating_high = smean.cl.boot(rating)[3]) %>% 
  ungroup()
  
```

##### Surprise

```{r}
fit_brm_exp2_responsibility_surprise = brm(
  formula = rating ~ 1 + surprise + (1 + surprise | participant),
  data = df.exp2.regression,
  cores = 2,
  seed = 1,
  file = "cache/fit_brm_exp2_responsibility_surprise")

fit_brm_exp2_responsibility_surprise %>% 
  summary()
```

##### Pivotality + n_causes + surprise 

```{r}
fit_brm_exp2_responsibility_bayesian = brm(
  formula = rating ~ 1 + pivotality + n_causes + surprise + (1 + pivotality + n_causes + surprise | participant),
  data = df.exp2.regression,
  cores = 2,
  seed = 1,
  file = "cache/fit_brm_exp2_responsibility_bayesian")
```

##### Pivotality + n_causes

```{r}
fit_brm_exp2_responsibility_importance = brm(
  formula = rating ~ 1 + pivotality + n_causes + (1 + pivotality + n_causes | participant),
  data = df.exp2.regression,
  cores = 2,
  seed = 1,
  file = "cache/fit_brm_exp2_responsibility_importance")

fit_brm_exp2_responsibility_importance %>% summary()
```

##### Pivotality + n_causes + surprise + outcome

```{r}
fit_brm_exp2_responsibility_bayesian_outcome = brm(
  formula = rating ~ 1 + pivotality + n_causes + surprise + outcome + (1 + pivotality + n_causes + surprise + outcome | participant),
  data = df.exp2.regression,
  cores = 2,
  seed = 1,
  file = "cache/fit_brm_exp2_responsibility_bayesian_outcome")

fit_brm_exp2_responsibility_bayesian_outcome %>% summary()
```

### Model comparison 

#### For the selection of 24 cases 

##### r and RMSE

###### Based on empirical ratings

```{r}

df.data = df.exp2.regression %>%
  left_join(df.exp1.trialinfo %>% 
              select(trial_exp1 = trial, trial = trial_exp2, person),
            by = c("trial", "person")) %>% 
  na.omit() %>% 
  left_join(df.exp1.regression %>% 
              select(trial, question, rating_mean) %>% 
              spread(question, rating_mean) %>% 
              rename(importance_empirical = importance,
                     surprise_empirical = surprise),
            by = c("trial_exp1" = "trial")) %>% 
  group_by(trial) %>% 
  summarize(rating = mean(rating),
            surprise_empirical = mean(surprise_empirical),
            importance_empirical = mean(importance_empirical))

fit_brm_exp2_selection_empirical %>% 
  fitted(newdata = df.data,
         re_formula = NA) %>%
  as_tibble() %>% 
  bind_cols(df.data) %>% 
  clean_names() %>%
  summarize(r_empirical = cor(estimate, rating),
            rmse_empirical = rmse(estimate, rating)) %>% 
  mutate(across(.cols = everything(),
                .fns = ~round(., 2)))
```

###### Based on model predictions 

```{r}
df.tmp = df.exp2.selection %>% 
  distinct(trial, index) %>% 
  left_join(df.exp2.regression.means,
            by = c("trial", "index")) %>% 
  rename(rating = rating_mean)

df.data = fitted(newdata = df.tmp,
                 object = fit_brm_exp1_surprise_bayesian,
                 re_formula = NA) %>% 
  as_tibble() %>% 
  clean_names() %>% 
  select(surprise_model = estimate) %>% 
  bind_cols(fitted(newdata = df.tmp,
                   object = fit_brm_exp1_importance_bayesian,
                   re_formula = NA) %>% 
              as_tibble() %>% 
              clean_names() %>% 
              select(importance_model = estimate)) %>% 
  bind_cols(df.tmp)

fit_brm_exp2_selection_model %>% 
  fitted(newdata = df.data,
         re_formula = NA) %>%
  as_tibble() %>% 
  bind_cols(df.data) %>% 
  clean_names() %>% 
  summarize(r_model = cor(estimate, rating),
            rmse_model = rmse(estimate, rating)) %>% 
  print_table()
```

#### Overall 

##### r and RMSE 

```{r}
df.data = df.exp2.regression.means %>% 
  rename(rating = rating_mean)

# FULL MODEL 
fit_brm_exp2_responsibility_bayesian %>% 
  fitted(newdata = df.data,
         re_formula = NA) %>%
  as_tibble() %>% 
  clean_names() %>% 
  bind_cols(df.data) %>% 
  summarize(r = cor(estimate, rating),
            rmse = rmse(estimate, rating)) %>% 
  print_table()

# ONLY SURPRISE MODEL 
fit_brm_exp2_responsibility_surprise %>% 
  fitted(newdata = df.data,
         re_formula = NA) %>%
  as_tibble() %>% 
  clean_names() %>% 
  bind_cols(df.data) %>% 
  summarize(r = cor(estimate, rating),
            rmse = rmse(estimate, rating)) %>% 
  print_table()

# ONLY IMPORTANCE MODEL 
fit_brm_exp2_responsibility_importance %>% 
  fitted(newdata = df.data,
         re_formula = NA) %>%
  as_tibble() %>% 
  clean_names() %>% 
  bind_cols(df.data) %>% 
  summarize(r = cor(estimate, rating),
            rmse = rmse(estimate, rating)) %>% 
  print_table()

# FULL MODEL WITH OUTCOME 
fit_brm_exp2_responsibility_bayesian_outcome %>% 
  fitted(newdata = df.data,
         re_formula = NA) %>%
  as_tibble() %>% 
  clean_names() %>% 
  bind_cols(df.data) %>% 
  summarize(r = cor(estimate, rating),
            rmse = rmse(estimate, rating)) %>% 
  print_table()
  
```

##### loo

```{r}
fit_brm_exp2_responsibility_bayesian = add_criterion(fit_brm_exp2_responsibility_bayesian, 
                                                     criterion = c("loo", "waic"),
                                                     reloo = T)

fit_brm_exp2_responsibility_importance = add_criterion(fit_brm_exp2_responsibility_importance, 
                                                       criterion = c("loo", "waic"),
                                                       reloo = T)

fit_brm_exp2_responsibility_bayesian_outcome = add_criterion(fit_brm_exp2_responsibility_bayesian_outcome,
                                                             criterion = c("loo", "waic"),
                                                             reloo = T)

loo_compare(fit_brm_exp2_responsibility_bayesian,
            fit_brm_exp2_responsibility_importance)

loo_compare(fit_brm_exp2_responsibility_bayesian,
            fit_brm_exp2_responsibility_bayesian_outcome)
```

## Tables 

### Trial information

#### 3 committee cases 

```{r}
df.exp2.long %>% 
  filter(size == 3) %>% 
  select(trial, rule, outcome, person, party, vote) %>% 
  distinct() %>% 
  unite("party_vote", c(party, vote)) %>% 
  spread(person, party_vote) %>% 
  separate(`1`, into = c("p1", "v1")) %>% 
  separate(`2`, into = c("p2", "v2")) %>% 
  separate(`3`, into = c("p3", "v3")) %>% 
  mutate(outcome = factor(outcome,
                          levels = c("not passed", "passed"),
                          labels = 0:1)) %>% 
  select(trial, p1, p2, p3, v1, v2, v3, threshold = rule, outcome) %>% 
  # xtable(digits = 0) %>%
  # print(include.rownames = F,
  #       booktabs = T)
  kable() %>% 
  kable_styling(fixed_thead = T,
                bootstrap_options = "striped") %>% 
  scroll_box(height = "500px")
  
```

#### 5 committee cases

```{r}
df.exp2.long %>% 
  filter(size == 5) %>% 
  select(trial, rule, outcome, person, party, vote) %>% 
  distinct() %>% 
  unite("party_vote", c(party, vote)) %>% 
  spread(person, party_vote) %>% 
  separate(`1`, into = c("p1", "v1")) %>% 
  separate(`2`, into = c("p2", "v2")) %>% 
  separate(`3`, into = c("p3", "v3")) %>% 
  separate(`4`, into = c("p4", "v4")) %>% 
  separate(`5`, into = c("p5", "v5")) %>% 
  mutate(outcome = factor(outcome,
                          levels = c("not passed", "passed"),
                          labels = 0:1)) %>% 
  select(trial, p1, p2, p3, p4, p5, v1, v2, v3, v4, v5, threshold = rule, outcome) %>% 
  # xtable(digits = 0) %>%
  # print(include.rownames = F,
  #       booktabs = T)
  kable() %>% 
  kable_styling(fixed_thead = T,
                bootstrap_options = "striped") %>% 
  scroll_box(height = "500px")
```


### Bayesian mixed effects model 

#### Overall 

```{r, warning=F, message=F}
fit_brm_exp2_responsibility_bayesian %>% 
  tidy(effects = "fixed",
       conf.method = "HPDinterval",
       fix.intercept = F) %>% 
  select(-c(effect, component)) %>% 
  rename(`lower 95% CI` = conf.low,
         `upper 95% CI` = conf.high) %>% 
  print_table()
```

## Plots 

### Scatterplot 

#### Overall 

```{r fig.height=5, fig.width=5}
df.plot = fit_brm_exp2_responsibility_bayesian %>% 
  fitted(newdata = df.exp2.regression.means,
         re_formula = NA) %>% 
  as_tibble() %>% 
  clean_names() %>%
  bind_cols(df.exp2.regression.means) %>%
  arrange(desc(trial)) %>% 
  rename(rating = rating_mean)

x = "estimate"
y = "rating"

ggplot(data = df.plot,
       mapping = aes(x = .data[[x]],
                     y = .data[[y]])) +
  geom_abline(intercept = 0,
              slope = 1,
              linetype = 2) +
  geom_smooth(mapping = aes(y = .data[[x]],
                            ymin = q2_5,
                            ymax = q97_5),
              stat = "identity",
              color = "lightblue",
              alpha = 0.4,
              fill = "lightblue") +
  geom_linerange(mapping = aes_string(ymin = str_c(y, "_low"),
                               ymax = str_c(y, "_high")),
                 alpha = 0.1) +
  geom_point(size = 2,
             alpha = 0.2) +
  annotate(geom = "text",
           label = df.plot %>% 
             summarize(r = cor(.data[[x]], .data[[y]]),
                       rmse = rmse(.data[[x]], .data[[y]])) %>% 
             mutate(across(.fns = ~ round(., 2))) %>% 
             unlist() %>% 
             str_c(names(.), " = ", .),
           x = c(0, 0), 
           y = c(100, 90),
           size = 7,
           hjust = 0) + 
  labs(x = "model prediction",
       y = "mean responsibility rating") +
  scale_x_continuous(breaks = seq(0, 100, 25)) +
  scale_y_continuous(breaks = seq(0, 100, 25)) +
  coord_cartesian(xlim = c(0, 100),
                  ylim = c(0, 100)) +
  theme(text = element_text(size = 24))

# ggsave(file = "../../figures/plots/experiment2_overall_scatter.pdf",
#        width = 5,
#        height = 5)
```
#### Selection of cases 

```{r fig.height=5, fig.width=5}
df.data = df.exp2.regression.means %>%
  left_join(df.exp1.trialinfo %>% 
              select(trial_exp1 = trial, trial = trial_exp2, person),
            by = c("trial", "person")) %>% 
  na.omit() %>% 
  left_join(df.exp1.regression %>% 
              select(trial, question, rating_mean) %>% 
              spread(question, rating_mean) %>% 
              rename(importance_empirical = importance,
                     surprise_empirical = surprise),
            by = c("trial_exp1" = "trial")) %>% 
  select(trial, trial_exp1, person, rating = rating_mean, rating_low, rating_high, contains("empirical"))

df.plot = fit_brm_exp2_selection_empirical %>% 
  fitted(newdata = df.data,
         re_formula = NA) %>%
  as_tibble() %>% 
  bind_cols(df.data) %>% 
  clean_names()

x = "estimate"
y = "rating"

ggplot(data = df.plot,
       mapping = aes(x = .data[[x]],
                     y = .data[[y]])) +
  geom_abline(intercept = 0,
              slope = 1,
              linetype = 2) +
  geom_smooth(mapping = aes(y = .data[[x]],
                            ymin = q2_5,
                            ymax = q97_5),
              stat = "identity",
              color = "lightblue",
              alpha = 0.4,
              fill = "lightblue") +
  geom_linerange(mapping = aes_string(ymin = str_c(y, "_low"),
                               ymax = str_c(y, "_high")),
                 alpha = 0.2) +
  geom_point(size = 2,
             alpha = 1) +
  annotate(geom = "text",
           label = df.plot %>% 
             summarize(r = cor(.data[[x]], .data[[y]]),
                       rmse = rmse(.data[[x]], .data[[y]])) %>% 
             mutate(across(.fns = ~ round(., 2))) %>% 
             unlist() %>% 
             str_c(names(.), " = ", .),
           x = c(0, 0), 
           y = c(100, 90),
           size = 7,
           hjust = 0) + 
  labs(x = "model prediction",
       y = "mean responsibility rating") +
  scale_x_continuous(breaks = seq(0, 100, 25)) +
  scale_y_continuous(breaks = seq(0, 100, 25)) +
  coord_cartesian(xlim = c(0, 100),
                  ylim = c(0, 100)) +
  theme(text = element_text(size = 24))

# ggsave(file = "../../figures/plots/experiment2_selection_scatter.pdf",
#        width = 5,
#        height = 5)
```



### Means for selection of cases 

```{r fig.height=12, fig.width=8}
# \u21e6 = left arrow
# \u21e8 = right arrow
# \u2713 = check mark 
# \u2717 = cross mark 
# \n = line break

# USING THE BAYESIAN MIXED MODEL RESULTS 

df.data = df.exp2.regression.means %>%
  left_join(df.exp1.trialinfo %>% 
              select(trial_exp1 = trial, trial = trial_exp2, person),
            by = c("trial", "person")) %>% 
  na.omit() %>% 
  left_join(df.exp1.regression %>% 
              select(trial, question, rating_mean) %>% 
              spread(question, rating_mean) %>% 
              rename(importance_empirical = importance,
                     surprise_empirical = surprise),
            by = c("trial_exp1" = "trial")) %>% 
  select(trial, trial_exp1, person, rating = rating_mean, rating_low, rating_high, contains("empirical"))

df.plot = fit_brm_exp2_selection_empirical %>% 
  fitted(newdata = df.data,
         re_formula = NA) %>%
  as_tibble() %>% 
  bind_cols(df.data) %>% 
  clean_names() %>% 
  # model predictions 
  left_join(df.exp2.predictions,
            by = c("trial", "person")) %>% 
  arrange(desc(rating)) %>% 
  mutate(index = 1:n()) %>% 
  rowwise() %>%
  mutate(n_same_no = n_same - n_same_yes,
         n_other_no = n_other - n_other_yes,
    label = str_c(index, ". ", "T=", rule, " \u21e8",
                       ifelse(party == 1, "S:", "O:"),
                       ifelse(vote == 1, "\u2713 ", "\u2717 "),
                  ifelse(n_same_yes != 0, str_c(rep("S:\u2713 ", n_same_yes), collapse = ""), ""),
                  ifelse(n_same_no != 0, str_c(rep("S:\u2717 ", n_same_no), collapse = ""), ""),
                  ifelse(n_other_yes != 0, str_c(rep("O:\u2713 ", n_other_yes), collapse = ""), ""),
                  ifelse(n_other_no != 0, str_c(rep("O:\u2717 ", n_other_no), collapse = ""), ""))) %>% 
  ungroup() %>% 
  rename(prediction = estimate,
         rating_mean = rating) %>% 
  select(trial_exp2 = trial, trial = trial_exp1, everything())

ggplot(data = df.plot,
       mapping = aes(x = reorder(label, rating_mean),
                     y = rating_mean)) +
  # confidence intervals 
  geom_linerange(aes(ymin = rating_low,
                     ymax = rating_high),
                 size = 1) +
  geom_point(aes(color = "responsibility"),
             size = 4) +
  geom_point(aes(y = prediction,
                 color = "model prediction"),
             size = 4) + 
  geom_point(aes(y = importance_empirical,
                 color = "importance"),
             size = 3
             ) + 
  geom_point(aes(y = surprise_empirical,
                 color = "surprise"),
             size = 3) + 
  scale_color_manual(breaks = c("surprise","importance","model prediction", "responsibility"),
                     values = c(surprise = "#e41a1c", 
                                importance = "#377eb8", 
                                `model prediction` = "gray80", 
                                responsibility = "black")) + 
  scale_y_continuous(breaks = seq(0, 100, 25),
                     labels = seq(0, 100, 25),
                     expand = c(0, 0)) + 
  labs(y = "judgments",
       color = "",
       caption = "T = threshold, S = same party, O = other party, \u21e8 = focus, \u2713 = yes, \u2717 = no") + 
  coord_flip(ylim = c(0, 100)) +
  geom_vline(xintercept = seq(0.5, 24.5, 1),
             color = "gray90") + 
  theme(axis.title.y = element_blank(),
        plot.margin = margin(r = 1,
                             b = 0.2,
                             l = 0.2,
                             unit = "cm"),
        text = element_text(size = 20),
        legend.position = "top",
        axis.text.y = element_text(family = "Arial Unicode MS",
                                   hjust = 0),
        plot.caption = element_text(family = "Arial Unicode MS",
                                    hjust = 1,
                                    margin = margin(t = 0.5, unit = "cm"),
                                    size = 16)) +
  guides(color = guide_legend(ncol = 2))

# ggsave(file = "../../figures/plots/experiment2_selection.pdf",
#        width = 8,
#        height = 12,
#        device = cairo_pdf)
```


# Experiment 3: Moral judgments

## Read in and structure data 

```{r}
# rating data
df.exp3 = read.csv("../../data/experiment_3/experiment_3.csv", fileEncoding = "UTF-8-BOM") %>% 
  clean_names()

# trial information 
df.exp3.trialinfo = df.exp1.trialinfo %>% 
  filter(str_detect(original_experiment, "Antonia")) %>% 
  select(-c(trial_exp2, person)) %>% 
  mutate(trial_exp3 = c(2, 1, 3, 4, 5))

# model predictions 
df.exp3.predictions = df.exp1.predictions %>% 
  filter(trial %in% df.exp3.trialinfo$trial) %>% 
  left_join(df.exp3.trialinfo %>% 
              select(trial, trial_exp3),
            by = "trial") %>% 
  filter(focus == person) %>% 
  rename(n_other_yes = sum,
         n_other = party) %>% 
  mutate(n_other = 4,
         n_other_yes = n_other_yes - 1) %>% 
  left_join(df.predictions %>% 
              select(vote_other,
                     n_other_yes,
                     n_other),
            by = c("n_other_yes",
                   "n_other")) %>% 
  mutate(surprise = 1 - vote_other)

# combine data frames 
df.exp3.long = df.exp3 %>% 
  select(participant, condition, x1:x5) %>% 
  gather("trial", "rating", -c(participant, condition)) %>% 
  mutate(trial = str_remove(trial,"x"),
         trial = as.numeric(trial)) %>% 
  mutate(condition = factor(condition,
                            levels = c(0, 1, 3),
                            labels = c("neutral",
                                       "moral",
                                       "wrongness")))

# regression data frame 
df.exp3.regression = df.exp3.long %>% 
  group_by(trial, condition) %>% 
  summarize(rating_mean = mean(rating),
            rating_low = smean.cl.boot(rating)[2],
            rating_high = smean.cl.boot(rating)[3]) %>%
  ungroup() %>% 
  left_join(df.exp3.predictions %>% 
              select(trial = trial_exp3, outcome, pivotality:norm_disposition),
            by = "trial") %>% 
  arrange(condition, trial)
```

## Demographic data 

```{r}
df.exp3.demographics = df.exp3 %>% 
  select(participant, duration, gender, age, strategy) %>% 
  mutate(gender = case_when(gender == 1 ~ "male",
                            gender == 2 ~ "female",
                            gender == 3 ~ "unspecified"))

df.exp3.demographics %>% 
  summarize(age_mean = round(mean(age)),
            age_sd = round(sd(age)),
            n_total = n(),
            n_female = sum(gender == "female"),
            n_male = sum(gender == "male"),
            n_unspecified = sum(gender == "unspecified"),
            time_mean = round(mean(duration)/60, 2),
            time_sd = round(sd(duration)/60, 2)) %>% 
  print_table()
```

### Participant feedback 

```{r}
df.exp3.demographics %>% 
  select(participant, strategy) %>% 
  datatable()
```

## Stats 

### Means 

#### Ratings by condition 

```{r}
df.exp3.long %>% 
  group_by(condition) %>% 
  summarize(mean = mean(rating),
            sd = sd(rating)) %>% 
  print_table()
```


### Bayesian regression 

#### Effect of condition on judgments 

```{r}
df.data = df.exp3.long %>% 
  left_join(df.exp3.predictions,
            by = c("trial" = "trial_exp3"))

# model with condition 
fit_brm_exp3_importance_overall = brm(
  formula = rating ~ (pivotality + n_causes) * condition + (1 | participant),
  data = df.data,
  cores = 2,
  seed = 1,
  file = "cache/fit_brm_exp3_importance_overall"
)

# model without condition
fit_brm_exp3_importance_no_condition = brm(
  formula = rating ~ pivotality + n_causes + (1 | participant),
  data = df.data,
  cores = 2,
  seed = 1,
  file = "cache/fit_brm_exp3_importance_no_condition"
)

# adding loo criterion 
fit_brm_exp3_importance_overall = add_criterion(fit_brm_exp3_importance_overall,
                                                criterion = "loo",
                                                reloo = T)

fit_brm_exp3_importance_no_condition = add_criterion(fit_brm_exp3_importance_no_condition,
                                                     criterion = "loo",
                                                     reloo = T)

# model comparison 
loo_compare(fit_brm_exp3_importance_overall,
            fit_brm_exp3_importance_no_condition)
```

#### Estimates for importance predictors separated by condition

```{r, warning=F, message=F}
func_brm_exp3 = function(data, condition){
  brm(formula = rating ~ pivotality + n_causes + (1 | participant),
      data = data %>% 
        filter(condition == condition),
      cores = 2,
      seed = 1,
      file = str_c("cache/fit_brm_exp3_", condition))
}

df.data = df.exp3.long %>% 
  left_join(df.exp3.predictions,
            by = c("trial" = "trial_exp3")) %>% 
  arrange(participant) %>% 
  group_by(condition) %>% 
  nest() %>% 
  arrange(condition) %>% 
  mutate(brm = map2(.x = data, .y = condition, ~ func_brm_exp3(.x, .y)))

tmp = map(1:3, ~ df.data$brm[[.]] %>% 
      tidy(effects = "fixed",
           conf.level = 0.95,
           conf.method = "HPDinterval",
           fix.intercept = F) %>% 
      select(-c(effect, component)))

print_table(tmp[[1]])
print_table(tmp[[2]])
print_table(tmp[[3]])
```

## Plots 

### Bars with mean judgments

- https://stackoverflow.com/questions/12409960/ggplot2-annotate-outside-of-plot

```{r}
# \u21e6 = left arrow
# \u21e8 = right arrow
# \u2713 = check mark 
# \u2717 = cross mark 
# \n = line break

x_labels = c(
  "1. T=1 \u21e8 \u2713 \u2717 \u2717 \u2717 \u2717",
  "2. T=1 \u21e8 \u2713 \u2713 \u2717 \u2717 \u2717",
  "3. T=1 \u21e8 \u2713 \u2713 \u2713 \u2713 \u2717",
  "4. T=2 \u21e8 \u2713 \u2713 \u2713 \u2717 \u2717",
  "5. T=2 \u21e8 \u2713 \u2713 \u2713 \u2713 \u2713")

df.plot = df.exp3.regression %>% 
  mutate(labels = factor(trial,
                         labels = x_labels))

ggplot(data = df.plot, 
       mapping = aes(x = reorder(labels,
                                 desc(labels)),
                     y = rating_mean)) + 
  geom_hline(yintercept = seq(25, 100, 25),
             linetype = 1,
             color = "gray80") + 
  geom_bar(stat = "identity",
           color = "black",
           fill = "#377eb8") + 
  geom_linerange(mapping = aes(ymin = rating_low,
                               ymax = rating_high),
                 size = 1) +
  geom_text(data = tibble(label = factor(c("neutral condition",
                                    "moral condition",
                                    "wrongness condition")),
                          condition = factor(c("neutral", "moral", "wrongness")),
                          x = Inf,
                          y = 0),
            mapping = aes(label = label,
                          x = x,
                          y = y),
            vjust = -1,
            hjust = 0,
            fontface = "bold",
            size = 4.5) +
  # geom_text(aes(label = "test", x = Inf, y = Inf), hjust = -1) + 
  geom_hline(yintercept = -0.5,
             size = 1) +
  facet_grid(cols = vars(condition),
             scales = "free",
             switch = "both",
             labeller = as_labeller(c(neutral = "responsibility",
                                      moral = "responsibility",
                                      wrongness = "wrongness"))) + 
  labs(y = "mean rating",
       caption = "T = threshold, \u21e8 = focus, \u2713 = yes, \u2717 = no") +
  scale_y_continuous(limits = c(-0.5, 100),
                     expand = c(0, 0)) + 
  coord_flip(clip = "off") +
  theme(text = element_text(size = 20),
        axis.text.y = element_text(family = "Arial Unicode MS",
                                   hjust = 0),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(size = 16),
        panel.spacing.x = unit(1, "cm"),
        axis.line.y = element_blank(),
        plot.margin = margin(t = 0.8, r = 0.8, b = 0.1, l = 0.2, unit = "cm"),
        plot.caption = element_text(family = "Arial Unicode MS",
                                    # color = "gray20",
                                    hjust = 1,
                                    margin = margin(t = 0.5, unit = "cm")),
        strip.placement = "outside") 

# ggsave(file = "../../figures/plots/experiment3_bars.pdf",
#        width = 8,
#        height = 5,
#        device = cairo_pdf)
```

# Experiment 4: Importance, surprise, responsibility, wrongfulness 

## Read in data 

```{r, message=F, warning=F}
set.seed(1)

df.exp4 = read_csv("../../data/experiment_4/experiment_4.csv") %>% 
  clean_names() %>% 
  filter(row_number() > 2) %>% 
  filter(!is.na(prolific_id)) %>% 
  mutate(participant = 1:n()) %>% 
  relocate(participant)

# trial information 
df.exp4.trialinfo = read_csv("data/experiment4_scenarios.csv") %>% 
  rename(bad = badness)

# replace incorrectly coded trial 
df.exp4.trialinfo = df.exp4.trialinfo %>% 
  mutate(votes = ifelse(trial == 10, 2, votes))

# main data 
df.exp4.long = df.exp4 %>% 
  select(participant, x1_b_1:x9_w_1) %>% 
  rename_with(.fn = ~ str_remove(., "_1")) %>% 
  pivot_longer(cols = -participant,
               names_to = c("trial", "question"),
               names_sep = "_",
               values_to = "rating") %>% 
  rename(trial_label = trial) %>% 
  mutate(trial_label = as.numeric(str_remove(trial_label, "x")),
         question = factor(question,
                           levels = c("b", "i", "s", "r", "w"),
                           labels = c("badness",
                                      "importance",
                                      "surprise",
                                      "responsibility",
                                      "wrongfulness"))) %>% 
  ungroup() %>% 
  left_join(df.exp4.trialinfo %>% 
              select(-policy),
            by = "trial_label") %>% 
  mutate(pivotality = 1/(votes - 1),
         rating = as.numeric(rating)) %>% 
  select(trial, everything(), -trial_label) %>% 
  arrange(participant, trial)

# format for regression 
df.exp4.regression = df.exp4.long %>%
  pivot_wider(names_from = question,
              values_from = rating)

# mean judgments 
df.exp4.regression.means = df.exp4.regression %>% 
  group_by(trial, bad, votes, pivotality) %>% 
  summarize(across(.cols = badness:wrongfulness,
                   .fns = ~ smean.cl.boot(.))) %>% 
  mutate(index = c("mean", "low", "high")) %>% 
  ungroup() %>%
  pivot_wider(names_from = index,
              values_from = badness:wrongfulness) %>% 
  rename_with(.fn = ~ str_remove(., "_mean")) %>% 
  mutate(badness_dummy = ifelse(badness < median(badness), "neutral", "bad"))

df.exp4.regression = df.exp4.regression %>% 
  left_join(df.exp4.regression.means %>% 
              select(trial, badness_dummy),
            by = "trial")

# regression with scaled predictors
df.exp4.regression.scaled = df.exp4.regression %>% 
  group_by(participant) %>% 
  mutate(across(.cols = c(badness, importance, surprise, responsibility, wrongfulness),
                .fns = ~ scale(.))) %>% 
  ungroup()

# mean scaled judgments 
df.exp4.regression.scaled.means = df.exp4.regression.scaled %>% 
  group_by(trial, bad, votes, pivotality) %>% 
  summarize(across(.cols = badness:wrongfulness,
                   .fns = ~ smean.cl.boot(.))) %>% 
  mutate(index = c("mean", "low", "high")) %>% 
  ungroup() %>%
  pivot_wider(names_from = index,
              values_from = badness:wrongfulness) %>% 
  rename_with(.fn = ~ str_remove(., "_mean"))
```

## Demographic data 

```{r}
df.exp4.demographics = df.exp4 %>% 
  select(participant, duration = duration_in_seconds, gender, age = age_1_text,
         race = race_1_text,importance, surprise) %>% 
  mutate(gender = tolower(gender)) %>% 
  mutate(across(c(duration, age), ~ as.numeric(.)))

df.exp4.demographics %>% 
  summarize(age_mean = round(mean(age)),
            age_sd = round(sd(age)),
            n_total = n(),
            n_female = sum(gender == "female"),
            n_male = sum(gender == "male"),
            n_unspecified = sum(gender == "unspecified"),
            time_mean = round(mean(duration)/60, 2),
            time_sd = round(sd(duration)/60, 2)) %>% 
  print_table()
```

## Stats 

### Badness ratings 

```{r}
df.exp4.regression.means %>% 
  select(trial, contains("bad")) %>% 
  left_join(df.exp4.trialinfo %>% 
              select(trial, policy),
            by = "trial") %>% 
  relocate(policy, .after = trial) %>% 
  arrange(bad) %>% 
  print_table()
```


### Confirmatory analysis 

#### Hypothesis 1: Pivotality predicts importance

The fewer politicians voted in favor of the policy, the more important a politicians vote who voted in favor is judged to be. 

```{r, warning=F}
fit.brm_exp4_pivotality_importance = brm(
  formula = importance ~ 1 + pivotality + (1 + pivotality | participant),
  data = df.exp4.regression,
  seed = 1,
  cores = 2,
  file = "cache/fit_brm_exp4_pivotality_importance")

fit.brm_exp4_pivotality_importance
```

#### Hypothesis 2: Moral badness predicts surprise 

The more morally negative a policy is perceived, the more surprising a vote in favor of that policy is judged to be. 

```{r, warning=F}
fit.brm_exp4_surprise_badness = brm(
  formula = surprise ~ 1 + badness + (1 + badness | participant),
  data = df.exp4.regression,
  seed = 1,
  cores = 2,
  file = "cache/fit_brm_exp4_surprise_badness")

fit.brm_exp4_surprise_badness
```

#### Hypothesis 3: Importance and surprise predict responsibility 

Both judgments of importance and surprise predict judgments of responsibility. 

```{r, warning=FALSE}
fit.brm_exp4_responsibility_importance_surprise = brm(
  formula = responsibility ~ 1 + importance + surprise + (1 + importance + surprise | participant),
  data = df.exp4.regression,
  seed = 1,
  cores = 2,
  iter = 4000,
  file = "cache/fit_brm_exp4_responsibility_importance_surprise")

fit.brm_exp4_responsibility_importance_surprise
```

#### Hypothesis 4: Moral badness interacts with importance and surprise to predict responsibility 

When judging responsibility, importance matters more for policies that are perceived as morally neutral (compared to morally negative policies), and surprise matters more for policies that are perceived as morally negative (compared to morally neutral policies). 

```{r, warning=FALSE}
fit.brm_exp4_responsibility_badness_dummy = brm(
  formula = responsibility ~ 1 + badness_dummy * (importance + surprise) + (1 + badness_dummy * (importance + surprise) | participant),
  data = df.exp4.regression %>% 
    mutate(badness_dummy = ifelse(badness_dummy == "bad", -0.5, 0.5)),
  seed = 1,
  cores = 2,
  file = "cache/fit_brm_exp4_responsibility_badness_dummy")

fit.brm_exp4_responsibility_badness_dummy
```

#### Hypothesis 5: Importance matters more for responsibility compared to wrongness 

Perceived importance matters more for judgments of responsibility compared to wrongness judgments. Note that participants who gave the same response for one of these questions across all trials are removed from this analysis (because of z-scoring on the individual participant level). 

```{r}
# responsibility 
fit.brm_exp4_responsibility_importance_surprise_scaled = brm(
  formula = responsibility ~ 1 + importance + surprise + (1 + importance + surprise | participant),
  data = df.exp4.regression.scaled,
  seed = 1,
  cores = 2,
  file = "cache/fit_brm_exp4_responsibility_importance_surprise_scaled")

fit.brm_exp4_responsibility_importance_surprise_scaled

# wrongfulness 
fit.brm_exp4_wrongfulness_importance_surprise_scaled = brm(
  formula = wrongfulness ~ 1 + importance + surprise + (1 + importance + surprise | participant),
  data = df.exp4.regression.scaled,
  seed = 1,
  cores = 2,
  iter = 8000,
  control = list(adapt_delta = 0.99),
  file = "cache/fit_brm_exp4_wrongfulness_importance_surprise_scaled")

fit.brm_exp4_wrongfulness_importance_surprise_scaled
```

#### Model checks 

```{r}
fit.brm_exp4_pivotality_importance %>% 
  plot(N = 6,
       ask = F)

fit.brm_exp4_surprise_badness %>% 
  plot(N = 6,
       ask = F)

fit.brm_exp4_responsibility_importance_surprise %>% 
  plot(N = 5,
       ask = F)

fit.brm_exp4_responsibility_badness_dummy %>% 
  plot(N = 5,
       ask = F)

fit.brm_exp4_responsibility_importance_surprise_scaled %>% 
  plot(N = 5,
       ask = F)

fit.brm_exp4_wrongfulness_importance_surprise_scaled %>% 
  plot(N = 5,
       ask = F)
```

### Exploratory analysis 

#### Responsibility: the role of surprise and importance

```{r, eval=FALSE}
fit.brm_exp4_responsibility_importance = brm(
  formula = responsibility ~ 1 + importance + (1 + importance | participant),
  data = df.exp4.regression,
  seed = 1,
  cores = 2,
  iter = 4000,
  file = "cache/fit_brm_exp4_responsibility_importance")

fit.brm_exp4_responsibility_surprise = brm(
  formula = responsibility ~ 1 + surprise + (1 + surprise | participant),
  data = df.exp4.regression,
  seed = 1,
  cores = 2,
  iter = 4000,
  file = "cache/fit_brm_exp4_responsibility_surprise")

fit.brm_exp4_responsibility_importance = add_criterion(
  fit.brm_exp4_responsibility_importance,
  criterion = "loo",
  reloo = T,
  file = "cache/fit_brm_exp4_responsibility_importance")

fit.brm_exp4_responsibility_surprise = add_criterion(
  fit.brm_exp4_responsibility_surprise,
  criterion = "loo",
  reloo = T,
  file = "cache/fit_brm_exp4_responsibility_surprise")

fit.brm_exp4_responsibility_importance_surprise = add_criterion(
  fit.brm_exp4_responsibility_importance_surprise,
  criterion = "loo",
  reloo = T,
  file = "cache/fit_brm_exp4_responsibility_importance_surprise")

loo_compare(fit.brm_exp4_responsibility_importance,
            fit.brm_exp4_responsibility_surprise,
            fit.brm_exp4_responsibility_importance_surprise)

model_weights(fit.brm_exp4_responsibility_importance,
              fit.brm_exp4_responsibility_surprise,
              fit.brm_exp4_responsibility_importance_surprise)
```

#### Wrongfulness: the role of surprise and importance

```{r, eval=FALSE}
fit.brm_exp4_wrongfulness_importance = brm(
  formula = wrongfulness ~ 1 + importance + (1 + importance | participant),
  data = df.exp4.regression,
  seed = 1,
  cores = 2,
  iter = 4000,
  file = "cache/fit_brm_exp4_wrongfulness_importance")

fit.brm_exp4_wrongfulness_surprise = brm(
  formula = wrongfulness ~ 1 + surprise + (1 + surprise | participant),
  data = df.exp4.regression,
  seed = 1,
  cores = 2,
  iter = 4000,
  file = "cache/fit_brm_exp4_wrongfulness_surprise")

fit.brm_exp4_wrongfulness_importance_surprise = brm(
  formula = wrongfulness ~ 1 + importance + surprise + (1 + importance + surprise | participant),
  data = df.exp4.regression,
  seed = 1,
  cores = 2,
  iter = 4000,
  file = "cache/fit_brm_exp4_wrongfulness_importance_surprise")

fit.brm_exp4_wrongfulness_importance = add_criterion(
  fit.brm_exp4_wrongfulness_importance,
  criterion = "loo",
  reloo = T,
  file = "cache/fit_brm_exp4_wrongfulness_importance")

fit.brm_exp4_wrongfulness_surprise = add_criterion(
  fit.brm_exp4_wrongfulness_surprise,
  criterion = "loo",
  reloo = T,
  file = "cache/fit_brm_exp4_wrongfulness_surprise")

fit.brm_exp4_wrongfulness_importance_surprise = add_criterion(
  fit.brm_exp4_wrongfulness_importance_surprise,
  criterion = "loo",
  reloo = T,
  file = "cache/fit_brm_exp4_wrongfulness_importance_surprise")

loo_compare(fit.brm_exp4_wrongfulness_importance,
            fit.brm_exp4_wrongfulness_surprise,
            fit.brm_exp4_wrongfulness_importance_surprise)

model_weights(fit.brm_exp4_wrongfulness_importance,
              fit.brm_exp4_wrongfulness_surprise,
              fit.brm_exp4_wrongfulness_importance_surprise)
```

#### Individual participants analysis 

##### Individual participant regressions 

We remove participant 45 from the individual regression analysis because this participant gave a 0 surprise rating for all scenarios (leading to problems with model fitting). 

```{r}
df.exp4.regression %>% 
  group_by(participant) %>% 
  summarize(across(.cols = c(wrongfulness, importance, surprise),
                   .fns = list(sd = ~ sd(.),
                               mean = ~ mean(.))))
```

###### Responsibility

This code chunk takes a long time to compute but needs to be run once to generate the .RData file with individual data fits. The file was larger than 100mb so we couldn't upload it to github. 

```{r, eval=F}
# participant 45 is removed from this analysis because they gave a 0 surprise rating
# across all trials  
df.fit = df.exp4.regression %>% 
  filter(participant != 45)

# restrict priors to be positive 
priors = prior(normal(0, 10), class = b, lb = 0)

# initial model fits (used for compilation)
fit.brm_exp4_responsibility_baseline_individual = 
  brm(formula = responsibility ~ 1,
      data = df.fit %>% 
        filter(participant == 1),
      cores = 4,
      chains = 4,
      iter = 4000,
      seed = 1,
      control = list(adapt_delta = 0.95),
      save_pars = save_pars(all = T),
      file = str_c("cache/fit_brm_exp4_responsibility_baseline_individual"))

fit.brm_exp4_responsibility_importance_individual = 
  brm(formula = responsibility ~ 1 + importance,
      data = df.fit %>% 
        filter(participant == 1),
      prior = priors,
      cores = 4,
      chains = 4,
      iter = 4000,
      seed = 1,
      control = list(adapt_delta = 0.95),
      save_pars = save_pars(all = T),
      file = str_c("cache/fit_brm_exp4_responsibility_importance_individual"))

fit.brm_exp4_responsibility_surprise_individual = 
  brm(formula = responsibility ~ 1 + surprise,
      data = df.fit %>% 
        filter(participant == 1),
      prior = priors,
      cores = 4,
      chains = 4,
      iter = 4000,
      seed = 1,
      control = list(adapt_delta = 0.95),
      save_pars = save_pars(all = T),
      file = str_c("cache/fit_brm_exp4_responsibility_surprise_individual"))

fit.brm_exp4_responsibility_importance_surprise_individual = 
  brm(formula = responsibility ~ 1 + importance + surprise,
      data = df.fit %>% 
        filter(participant == 1),
      prior = priors,
      cores = 4,
      chains = 4,
      iter = 4000,
      seed = 1,
      control = list(adapt_delta = 0.95),
      save_pars = save_pars(all = T),
      file = str_c("cache/fit_brm_exp4_responsibility_importance_surprise_individual"))

# update model fits for all participants
df.exp4.responsibility_individual_fit = df.fit %>%
  group_by(participant) %>%
  nest() %>%
  ungroup() %>% 
  # fit model for each participant
  mutate(fit_baseline = map(
    .x = data,
    .f = ~ update(fit.brm_exp4_responsibility_baseline_individual,
                  newdata = .x,
                  seed = 1)),
    fit_importance = map(
      .x = data,
      .f = ~ update(fit.brm_exp4_responsibility_importance_individual,
                    newdata = .x,
                    seed = 1)),
    fit_surprise = map(
      .x = data,
      .f = ~ update(fit.brm_exp4_responsibility_surprise_individual,
                    newdata = .x,
                    seed = 1)),
    fit_importance_surprise = map(
      .x = data,
      .f = ~ update(fit.brm_exp4_responsibility_importance_surprise_individual,
                    newdata = .x,
                    seed = 1))) %>% 
  mutate(fit_baseline = map(fit_baseline, ~ add_criterion(.,
                                                          criterion = "loo",
                                                          moment_match = T)),
         fit_importance = map(fit_importance, ~ add_criterion(.,
                                                              criterion = "loo",
                                                              moment_match = T)),
         fit_surprise = map(fit_surprise, ~ add_criterion(.,
                                                          criterion = "loo",
                                                          moment_match = T)),
         fit_importance_surprise = map(fit_importance_surprise,
                                       ~ add_criterion(.,
                                                       criterion = "loo",
                                                       moment_match = T)),
         r_importance = map2_dbl(.x = data,
                                 .y = fit_importance,
                                 .f = ~ cor(.x$responsibility, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         r_surprise = map2_dbl(.x = data,
                                 .y = fit_surprise,
                                 .f = ~ cor(.x$responsibility, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         r_importance_surprise = map2_dbl(.x = data,
                                 .y = fit_importance_surprise,
                                 .f = ~ cor(.x$responsibility, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         rmse_baseline = map2_dbl(.x = data,
                                 .y = fit_baseline,
                                 .f = ~ rmse(.x$wrongfulness, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         rmse_importance = map2_dbl(.x = data,
                                 .y = fit_importance,
                                 .f = ~ rmse(.x$responsibility, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         rmse_surprise = map2_dbl(.x = data,
                                 .y = fit_surprise,
                                 .f = ~ rmse(.x$responsibility, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         rmse_importance_surprise = map2_dbl(.x = data,
                                 .y = fit_importance_surprise,
                                 .f = ~ rmse(.x$responsibility, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         model_comparison = pmap(.l = list(baseline = fit_baseline,
                                           importance = fit_importance,
                                           surprise = fit_surprise,
                                           importance_surprise = fit_importance_surprise),
                                 .f = ~ loo_compare(..1, ..2, ..3, ..4)),
         best_model = map_chr(.x = model_comparison,
                              .f = ~ rownames(.) %>% .[1]),
         best_model = factor(best_model,
                             levels = c("..1", "..2", "..3", "..4"),
                             labels = c("baseline",
                                        "importance",
                                        "surprise",
                                        "importance_surprise")))

save(list = c("df.exp4.responsibility_individual_fit"),
     file = "data/exp4_responsibility_individual_fit.RData")
```

###### Wrongfulness

This code chunk takes a long time to compute but needs to be run once to generate the .RData file with individual data fits. The file was larger than 100mb so we couldn't upload it to github. 

```{r, eval=F}
# participant 45 is removed from this analysis because they gave a 0 surprise rating
# across all trials  
df.fit = df.exp4.regression %>% 
  filter(participant != 45)

# restrict priors to be positive 
priors = prior(normal(0, 10), class = b, lb = 0)

# initial model fits (used for compilation)
fit.brm_exp4_wrongfulness_baseline_individual = 
  brm(formula = wrongfulness ~ 1,
      data = df.fit %>% 
        filter(participant == 1),
      cores = 4,
      chains = 4,
      iter = 4000,
      seed = 1,
      control = list(adapt_delta = 0.95),
      save_pars = save_pars(all = T),
      file = str_c("cache/fit_brm_exp4_wrongfulness_baseline_individual"))

fit.brm_exp4_wrongfulness_importance_individual = 
  brm(formula = wrongfulness ~ 1 + importance,
      data = df.fit %>% 
        filter(participant == 1),
      prior = priors,
      cores = 4,
      chains = 4,
      iter = 4000,
      seed = 1,
      control = list(adapt_delta = 0.95),
      save_pars = save_pars(all = T),
      file = str_c("cache/fit_brm_exp4_wrongfulness_importance_individual"))

fit.brm_exp4_wrongfulness_surprise_individual = 
  brm(formula = wrongfulness ~ 1 + surprise,
      data = df.fit %>% 
        filter(participant == 1),
      prior = priors,
      cores = 4,
      chains = 4,
      iter = 4000,
      seed = 1,
      control = list(adapt_delta = 0.95),
      save_pars = save_pars(all = T),
      file = str_c("cache/fit_brm_exp4_wrongfulness_surprise_individual"))

fit.brm_exp4_wrongfulness_importance_surprise_individual = 
  brm(formula = wrongfulness ~ 1 + importance + surprise,
      data = df.fit %>% 
        filter(participant == 1),
      prior = priors,
      cores = 4,
      chains = 4,
      iter = 4000,
      seed = 1,
      control = list(adapt_delta = 0.95),
      save_pars = save_pars(all = T),
      file = str_c("cache/fit_brm_exp4_wrongfulness_importance_surprise_individual"))

# update model fits for all participants
df.exp4.wrongfulness_individual_fit = df.fit %>%
  group_by(participant) %>%
  nest() %>%
  ungroup() %>% 
  # fit model for each participant
  mutate(fit_baseline = map(
    .x = data,
    .f = ~ update(fit.brm_exp4_wrongfulness_baseline_individual,
                  newdata = .x,
                  seed = 1)),
    fit_importance = map(
      .x = data,
      .f = ~ update(fit.brm_exp4_wrongfulness_importance_individual,
                    newdata = .x,
                    seed = 1)),
    fit_surprise = map(
      .x = data,
      .f = ~ update(fit.brm_exp4_wrongfulness_surprise_individual,
                    newdata = .x,
                    seed = 1)),
    fit_importance_surprise = map(
      .x = data,
      .f = ~ update(fit.brm_exp4_wrongfulness_importance_surprise_individual,
                    newdata = .x,
                    seed = 1))) %>% 
  mutate(fit_baseline = map(fit_baseline, ~ add_criterion(.,
                                                          criterion = "loo",
                                                          moment_match = T)),
         fit_importance = map(fit_importance, ~ add_criterion(.,
                                                              criterion = "loo",
                                                              moment_match = T)),
         fit_surprise = map(fit_surprise, ~ add_criterion(.,
                                                          criterion = "loo",
                                                          moment_match = T)),
         fit_importance_surprise = map(fit_importance_surprise,
                                       ~ add_criterion(.,
                                                       criterion = "loo",
                                                       moment_match = T)),
         r_importance = map2_dbl(.x = data,
                                 .y = fit_importance,
                                 .f = ~ cor(.x$wrongfulness, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         r_surprise = map2_dbl(.x = data,
                                 .y = fit_surprise,
                                 .f = ~ cor(.x$wrongfulness, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         r_importance_surprise = map2_dbl(.x = data,
                                 .y = fit_importance_surprise,
                                 .f = ~ cor(.x$wrongfulness, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         rmse_baseline = map2_dbl(.x = data,
                                 .y = fit_baseline,
                                 .f = ~ rmse(.x$wrongfulness, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         rmse_importance = map2_dbl(.x = data,
                                 .y = fit_importance,
                                 .f = ~ rmse(.x$wrongfulness, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         rmse_surprise = map2_dbl(.x = data,
                                 .y = fit_surprise,
                                 .f = ~ rmse(.x$wrongfulness, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         rmse_importance_surprise = map2_dbl(.x = data,
                                 .y = fit_importance_surprise,
                                 .f = ~ rmse(.x$wrongfulness, .y %>% 
                                              fitted() %>% 
                                              .[, 1])),
         model_comparison = pmap(.l = list(baseline = fit_baseline,
                                           importance = fit_importance,
                                           surprise = fit_surprise,
                                           importance_surprise = fit_importance_surprise),
                                 .f = ~ loo_compare(..1, ..2, ..3, ..4)),
         best_model = map_chr(.x = model_comparison,
                              .f = ~ rownames(.) %>% .[1]),
         best_model = factor(best_model,
                             levels = c("..1", "..2", "..3", "..4"),
                             labels = c("baseline",
                                        "importance",
                                        "surprise",
                                        "importance_surprise")))

save(list = c("df.exp4.wrongfulness_individual_fit"),
     file = "data/exp4_wrongfulness_individual_fit.RData")
```

##### Pairwise correlations between measures 

```{r, warning=FALSE}
df.exp4.individual.correlations = df.exp4.long %>%
  select(-c(bad, votes)) %>% 
  pivot_wider(names_from = question,
              values_from = rating) %>% 
  group_by(participant) %>% 
  summarize(r_pivotality_importance = cor(pivotality, importance),
            r_badness_surprise = cor(badness, surprise),
            r_importance_responsibility = cor(importance, responsibility),
            r_surprise_responsibility = cor(surprise, responsibility),
            r_importance_wrongfulness = cor(importance, wrongfulness),
            r_surprise_wrongfulness = cor(surprise, wrongfulness)) %>% 
  ungroup()
```

## Tables 

### Bayesian regression models 

#### 1. Pivotality predicts importance 

```{r}
fit.brm_exp4_pivotality_importance %>% 
  fun_brms_table(format = "latex")
```

#### 2. Badness predicts surprise

```{r}
fit.brm_exp4_surprise_badness %>% 
  fun_brms_table(format = "latex")
```

#### 3. Importance and surprise predict responsibility

```{r}
fit.brm_exp4_responsibility_importance_surprise %>% 
  fun_brms_table(format = "latex")
```

#### 4. Moral badness interacts with importance and surprise to predict responsibility 

```{r}
fit.brm_exp4_responsibility_badness_dummy %>% 
  fun_brms_table(format = "latex")
```

#### 5. Importance matters more for responsibility 

```{r}
fit.brm_exp4_responsibility_importance_surprise_scaled %>% 
    tidy() %>% 
    filter(effect == "fixed") %>% 
    select(term:conf.high) %>% 
    rename(`lower 95\\% HDI` = conf.low,
           `upper 95\\% HDI` = conf.high) %>% 
  mutate(response = "responsibility") %>% 
  bind_rows(
    fit.brm_exp4_wrongfulness_importance_surprise_scaled %>% 
    tidy() %>% 
    filter(effect == "fixed") %>% 
    select(term:conf.high) %>% 
    rename(`lower 95\\% HDI` = conf.low,
           `upper 95\\% HDI` = conf.high) %>% 
  mutate(response = "wrongfulness")) %>% 
  relocate(response) %>% 
  filter(term != "(Intercept)") %>% 
  print_table("latex")
```



### Correlations 

```{r, message=F}
df.exp4.long %>% 
  group_by(question, trial, bad, pivotality) %>% 
  summarize(mean = mean(rating)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = question,
              values_from = mean) %>% 
  select(-c(trial, bad)) %>% 
  relocate(responsibility, wrongfulness, surprise, importance, badness, pivotality) %>% 
  correlate() %>% 
  shave() %>% 
  fashion() %>% 
  as_tibble() %>% 
  select(-pivotality) %>% 
  filter(term != "responsibility") %>% 
  print_table()
```

### Individual participant correlations

```{r}
df.exp4.individual.correlations %>% 
  pivot_longer(cols = -participant,
               names_to = "index",
               values_to = "correlation") %>% 
  mutate(index = str_replace(index, "r_", "r("),
         index = str_replace(index, "_", ", "),
         index = str_c(index, ")"),
         index = factor(index, levels = 
                          c("r(pivotality, importance)",
                            "r(badness, surprise)",
                            "r(surprise, responsibility)",
                            "r(importance, responsibility)",
                            "r(surprise, wrongfulness)",
                            "r(importance, wrongfulness)"))) %>% 
  drop_na() %>% 
  group_by(index) %>% 
  summarize(median = median(correlation),
            `5 percentile` = quantile(correlation, 0.05),
            `95 percentile` = quantile(correlation, 0.95)) %>% 
  ungroup() %>% 
  mutate(across(-index, ~ round(., 2))) %>% 
  print_table()
```

### Individual participant model comparison 

```{r}
# Note that these two files aren't on the github repo.
# They need to be generated first by running the 
# `Individual participant model comparison` code chunks
load("data/exp4_responsibility_individual_fit.RData")
load("data/exp4_wrongfulness_individual_fit.RData")

# RESPONSIBILITY 
df.exp4.models.responsibility = tibble(model = c("baseline",
                             "importance",
                             "surprise",
                             "importance_surprise"),
                   formula = c("responsibility ~ 1",
                               "responsibility ~ 1 + importance",
                               "responsibility ~ 1 + surprise",
                               "responsibility ~ 1 + importance + surprise")) %>% 
  left_join(df.exp4.responsibility_individual_fit %>% 
              count(best_model),
            by = c("model" =  "best_model")) %>% 
  left_join(df.exp4.responsibility_individual_fit %>% 
              select(where(~ !is.list(.))) %>% 
              mutate(r_baseline = 0) %>% 
              pivot_longer(cols = -c(participant, best_model),
                           names_to = c("measure", "predictor"),
                           names_pattern = "(^[^_]+(?=_))_(.*)",
                           values_to = "value") %>% 
              filter(best_model == predictor) %>%
              group_by(best_model, measure) %>% 
              summarize(quantile = quantile(value,
                                            probs = c(0.25, 0.5, 0.75),
                                            na.rm = T)) %>% 
              mutate(index = c(0.25, 0.5, 0.75)) %>% 
              ungroup() %>% 
              pivot_wider(names_from = index,
                          values_from = quantile) %>% 
              mutate(across(where(is.numeric), ~ round(., 2))) %>% 
              mutate(value = str_c(`0.5`, " [", `0.25`, ", ", `0.75`, "]")) %>% 
              select(best_model, measure, value) %>% 
              pivot_wider(names_from = measure,
                          values_from = value),
            by = c("model" =  "best_model"))

# WRONGFULNESS 
df.exp4.models.wrongfulness = tibble(model = c("baseline",
                             "importance",
                             "surprise",
                             "importance_surprise"),
                   formula = c("wrongfulness ~ 1",
                               "wrongfulness ~ 1 + importance",
                               "wrongfulness ~ 1 + surprise",
                               "wrongfulness ~ 1 + importance + surprise")) %>% 
  left_join(df.exp4.wrongfulness_individual_fit %>% 
              count(best_model),
            by = c("model" =  "best_model")) %>% 
  left_join(df.exp4.wrongfulness_individual_fit %>% 
              select(where(~ !is.list(.))) %>% 
              mutate(r_baseline = 0) %>% 
              pivot_longer(cols = -c(participant, best_model),
                           names_to = c("measure", "predictor"),
                           names_pattern = "(^[^_]+(?=_))_(.*)",
                           values_to = "value") %>% 
              filter(best_model == predictor) %>%
              group_by(best_model, measure) %>% 
              summarize(quantile = quantile(value,
                                            probs = c(0.05, 0.5, 0.95),
                                            na.rm = T)) %>% 
              mutate(index = c(0.05, 0.5, 0.95)) %>% 
              ungroup() %>% 
              pivot_wider(names_from = index,
                          values_from = quantile) %>% 
              mutate(across(where(is.numeric), ~ round(., 2))) %>% 
              mutate(value = str_c(`0.5`, " [", `0.05`, ", ", `0.95`, "]")) %>% 
              select(best_model, measure, value) %>% 
              pivot_wider(names_from = measure,
                          values_from = value),
            by = c("model" =  "best_model"))

df.exp4.models.responsibility %>% 
  bind_rows(df.exp4.models.wrongfulness) %>% 
  # relocate(formula) %>% 
  print_table("latex")
```


```{r}
df.exp4.responsibility_individual_fit %>% 
  select(where(~ !is.list(.))) %>% 
  filter(best_model == "baseline") %>% 
  summarize()
  
```

### Scenarios 

```{r}
df.exp4.trialinfo %>% 
  select(trial, policy) %>% 
  mutate(trial = as.character(trial)) %>%
  rename(scenario = trial) %>% 
  print_table()
```

```{r, eval = F}
set.seed(1)

df.exp4.trialinfo %>% 
  select(trial, policy, votes) %>% 
  left_join(df.exp4.long %>% 
              filter(question == "badness") %>% 
              group_by(trial) %>% 
              summarize(value = round(smean.cl.boot(rating), 2)) %>% 
              mutate(name = c("mean", "low", "high")) %>% 
              pivot_wider() %>% 
              mutate(badness = str_c(mean, " [", low, ", ", high, "]")),
            by = "trial") %>% 
  arrange(mean) %>% 
  mutate(votes = as.character(votes),
         index = 1:n()) %>% 
  select(index, policy, badness, votes) %>% 
  print_table("latex")
```

### Averaged ratings across the scenarios 

```{r}
df.exp4.regression.means %>% 
  mutate(across(c(trial, votes), ~as.factor(.))) %>% 
  mutate(across(where(is.numeric), ~ round(., 2))) %>% 
  mutate(badness = str_c(badness, " [", badness_low, ", ", badness_high, "]"),
         importance = str_c(importance, " [", importance_low, ", ", importance_high, "]"),
         surprise = str_c(surprise, " [", surprise_low, ", ", surprise_high, "]"),
         responsibility = str_c(responsibility, " [", responsibility_low, ", ", responsibility_high, "]"),
         wrongfulness = str_c(wrongfulness, " [", wrongfulness_low, ", ", wrongfulness_high, "]")) %>% 
  select(scenario = trial, votes, badness, importance, surprise, responsibility, wrongfulness) %>% 
  # left_join(df.exp4.trialinfo %>% 
  #             select(trial, policy),
  #           by = "trial") %>% 
  # relocate(policy, .after = trial) %>% 
  print_table()

```



## Plots 

### 1. Votes predict importance 

```{r fig.height=4, fig.width=8}
set.seed(1)

df.plot = df.exp4.long %>% 
  filter(question == "importance")

ggplot(data = df.plot,
       mapping = aes(x = votes,
                     y = rating)) + 
  geom_point(alpha = 0.05,
             position = position_jitter(width = 0.05, height = 0)) +
  geom_smooth(method = "lm",
              se = F,
              formula = y ~ x,
              color = "red",
              fill = "red",
              size = 2) + 
  geom_smooth(method = "lm",
              se = F,
              formula = y ~ I(1/(x - 1)),
              color = "blue",
              fill = "blue",
              size = 2) + 
  stat_summary(fun.data = "mean_cl_boot",
               size = 1,
               shape = 21,
               fill = "white") +
  labs(y = "importance rating",
       x = "number of votes in favor") + 
  theme(text = element_text(size = 24),
        panel.grid.major.y = element_line())

# ggsave("../../figures/plots/experiment4_votes_importance.pdf",
#        width = 8,
#        height = 4)
```

### 2. Badness predicts surprise 

```{r fig.height=5, fig.width=5, message=FALSE, warning=FALSE}
set.seed(1)

# df.plot = df.exp4.regression.means

x = "badness"
y = "surprise"

df.plot = fit.brm_exp4_surprise_badness %>%
  fitted(newdata = df.exp4.regression.means,
         re_formula = NA) %>% 
  as_tibble() %>% 
  clean_names() %>%
  bind_cols(df.exp4.regression.means)

# scatter plot 
ggplot(data = df.plot,
       mapping = aes(x = .data[[x]],
                     y = .data[[y]])) + 
  geom_smooth(mapping = aes(y = estimate,
                            ymin = q2_5,
                            ymax = q97_5),
              stat = "identity",
              alpha = 0.4,
              color = "lightblue",
              fill = "lightblue") + 
  geom_abline(intercept = 0,
              slope = 1,
              linetype = 2) +
  geom_linerange(mapping = aes(xmin = .data[[str_c(x, "_low")]],
                               xmax = .data[[str_c(x, "_high")]]),
                 alpha = 0.2) + 
  geom_linerange(mapping = aes(ymin = .data[[str_c(y, "_low")]],
                               ymax = .data[[str_c(y, "_high")]]),
                 alpha = 0.2) + 
  geom_point(size = 2) +
  annotate(geom = "text",
           label = df.plot %>%
             summarize(r = cor(.data[[x]], .data[[y]]),
                       rmse = rmse(.data[[x]], .data[[y]])) %>%
             mutate(across(.fns = ~ round(., 2))) %>%
             unlist() %>%
             str_c(names(.), " = ", .),
           x = c(0, 0),
           y = c(100, 90),
           size = 7,
           hjust = 0) + 
  scale_x_continuous(breaks = seq(0, 100, 25)) +
  scale_y_continuous(breaks = seq(0, 100, 25)) +
  coord_cartesian(xlim = c(0, 100),
                  ylim = c(0, 100)) +
  theme(text = element_text(size = 24))

# ggsave("../../figures/plots/experiment4_badness_surprise.pdf",
#        width = 5,
#        height = 5)
```

### 3. Importance and surprise predict responsibility 

```{r fig.height=5, fig.width=5}
set.seed(1)

df.plot = fit.brm_exp4_responsibility_importance_surprise %>% 
  fitted(newdata = df.exp4.regression.means,
         re_formula = NA) %>% 
  as_tibble() %>% 
  clean_names() %>%
  bind_cols(df.exp4.regression.means)

x = "estimate"  
y = "responsibility"

ggplot(data = df.plot,
       mapping = aes(x = .data[[x]],
                     y = .data[[y]])) +
  geom_abline(intercept = 0,
              slope = 1,
              linetype = 2) +
  geom_smooth(mapping = aes(y = estimate,
                            ymin = q2_5,
                            ymax = q97_5),
              stat = "identity",
              color = "lightblue",
              alpha = 0.4,
              fill = "lightblue") +
  geom_linerange(mapping = aes_string(ymin = str_c(y, "_low"),
                               ymax = str_c(y, "_high")),
                 alpha = 0.2) +
  geom_point(size = 2,
             alpha = 1) +
  annotate(geom = "text",
           label = df.plot %>% 
             summarize(r = cor(.data[[x]], .data[[y]]),
                       rmse = rmse(.data[[x]], .data[[y]])) %>% 
             mutate(across(.fns = ~ round(., 2))) %>% 
             unlist() %>% 
             str_c(names(.), " = ", .),
           x = c(0, 0), 
           y = c(100, 90),
           size = 7,
           hjust = 0) + 
  labs(x = "surprise & importance",
       y = y) +
  scale_x_continuous(breaks = seq(0, 100, 25)) +
  scale_y_continuous(breaks = seq(0, 100, 25)) +
  coord_cartesian(xlim = c(0, 100),
                  ylim = c(0, 100)) +
  theme(text = element_text(size = 24))

# ggsave("../../figures/plots/experiment4_importance_surprise_responsibility.pdf",
#        width = 5,
#        height = 5)
```

### 4. Moral badness interaction 

```{r, warning=FALSE, message=FALSE}
ggpredict(fit.brm_exp4_responsibility_badness_dummy,
          terms = c("importance", "badness_dummy")) %>% 
  plot() +
  theme_classic() +
  labs(x = "importance", 
       y = "responsibility") + 
  scale_color_manual(labels = c("bad", "neutral"),
                     values = c("red", "blue")) +
  labs(color = "badness") + 
  scale_y_continuous(breaks = seq(0, 100, 25),
                     limits = c(0, 100)) + 
  theme(text = element_text(size = 24),
        plot.title = element_blank(),
        legend.position = c(0, 1),
        legend.justification = c(-0.25, 1)) +
  guides(color = guide_legend(override.aes = list(fill = NA)))

# ggsave("../../figures/plots/experiment4_responsibility_importance_dummy.pdf",
#        width = 5,
#        height = 5)
```

```{r, warning=FALSE}
ggpredict(fit.brm_exp4_responsibility_badness_dummy,
          terms = c("surprise", "badness_dummy")) %>% 
  plot() +
  theme_classic() +
  labs(x = "surprise", 
       y = "responsibility") + 
  scale_color_manual(labels = c("bad", "neutral"),
                     values = c("red", "blue")) +
  labs(color = "badness") + 
  scale_y_continuous(breaks = seq(0, 100, 25),
                     limits = c(0, 100)) + 
  theme(text = element_text(size = 24),
        plot.title = element_blank(),
        legend.position = c(0, 1),
        legend.justification = c(-0.25, 1)) +
  guides(color = guide_legend(override.aes = list(fill = NA)))

# ggsave("../../figures/plots/experiment4_responsibility_surprise_dummy.pdf",
#        width = 5,
#        height = 5)
```
Posterior distributions of the interaction terms. 

```{r}
fit.brm_exp4_responsibility_badness_dummy %>% 
  emtrends(specs = "badness_dummy",
           var = c("importance")) %>% 
  contrast(interaction = "consec") %>% 
  gather_emmeans_draws() %>% 
  ggplot(data = .,
         mapping = aes(x = .value,
                       fill = stat(x > 0))) + 
  stat_halfeye(show.legend = F) + 
  geom_vline(xintercept = 0,
             linetype = 2) + 
  scale_fill_manual(values = c("gray80", "lightblue")) +
  labs(title = "Difference in slopes for importance ratings",
       subtitle = "We correctly predicted a positive difference in slopes.")

fit.brm_exp4_responsibility_badness_dummy %>% 
  emtrends(specs = "badness_dummy",
           var = c("surprise")) %>% 
  contrast(interaction = "consec") %>% 
  gather_emmeans_draws() %>% 
  ggplot(data = .,
         mapping = aes(x = .value,
                       fill = stat(x < 0))) + 
  stat_halfeye(show.legend = F) + 
  geom_vline(xintercept = 0,
             linetype = 2) + 
  scale_fill_manual(values = c("gray80", "lightblue")) + 
  labs(title = "Difference in slopes for surprise ratings",
       subtitle = "We incorrectly predicted a negative difference in slopes.")
```

### 5. Importance matters more for responsibility 

#### Responsibility

```{r fig.height=5, fig.width=5, message=FALSE, warning=FALSE}
set.seed(1)

df.plot = fit.brm_exp4_responsibility_importance_surprise_scaled %>% 
  fitted(newdata = df.exp4.regression.scaled.means,
         re_formula = NA) %>% 
  as_tibble() %>% 
  clean_names() %>%
  bind_cols(df.exp4.regression.scaled.means)

y = "responsibility"

ggplot(data = df.plot,
       mapping = aes(x = estimate,
                     y = .data[[y]])) +
  geom_abline(intercept = 0,
              slope = 1,
              linetype = 2) +
  geom_smooth(mapping = aes(y = estimate,
                            ymin = q2_5,
                            ymax = q97_5),
              stat = "identity",
              color = "lightblue",
              alpha = 0.4,
              fill = "lightblue") +
  geom_linerange(mapping = aes_string(ymin = str_c(y, "_low"),
                               ymax = str_c(y, "_high")),
                 alpha = 0.2) +
  geom_point(size = 2) +
  annotate(geom = "text",
           label = df.plot %>%
             summarize(r = cor(.data[["estimate"]], .data[[y]]),
                       rmse = rmse(.data[["estimate"]], .data[[y]])) %>%
             mutate(across(.fns = ~ round(., 2))) %>%
             unlist() %>%
             str_c(names(.), " = ", .),
           x = c(-1.1, -1.1),
           y = c(1, 0.8),
           size = 7,
           hjust = 0) +
  labs(x = "surprise & importance",
       y = y) +
  scale_x_continuous(breaks = seq(-1, 1, 0.5)) +
  scale_y_continuous(breaks = seq(-1, 1, 0.5)) +
  coord_cartesian(xlim = c(-1.25, 1.25),
                  ylim = c(-1.25, 1.25)) +
  theme(text = element_text(size = 24))

# ggsave("../../figures/plots/experiment4_responsibility_surprise_importance_scaled.pdf",
#        width = 5,
#        height = 5)
```

#### Wrongfulness

```{r fig.height=5, fig.width=5, message=FALSE, warning=FALSE}
set.seed(1)

df.plot = fit.brm_exp4_wrongfulness_importance_surprise_scaled %>% 
  fitted(newdata = df.exp4.regression.scaled.means,
         re_formula = NA) %>% 
  as_tibble() %>% 
  clean_names() %>%
  bind_cols(df.exp4.regression.scaled.means)

y = "wrongfulness"

ggplot(data = df.plot,
       mapping = aes(x = estimate,
                     y = .data[[y]])) +
  geom_abline(intercept = 0,
              slope = 1,
              linetype = 2) +
  geom_smooth(mapping = aes(y = estimate,
                            ymin = q2_5,
                            ymax = q97_5),
              stat = "identity",
              color = "lightblue",
              alpha = 0.4,
              fill = "lightblue") +
  geom_linerange(mapping = aes_string(ymin = str_c(y, "_low"),
                               ymax = str_c(y, "_high")),
                 alpha = 0.2) +
  geom_point(size = 2) +
  annotate(geom = "text",
           label = df.plot %>%
             summarize(r = cor(.data[["estimate"]], .data[[y]]),
                       rmse = rmse(.data[["estimate"]], .data[[y]])) %>%
             mutate(across(.fns = ~ round(., 2))) %>%
             unlist() %>%
             str_c(names(.), " = ", .),
           x = c(-1.1, -1.1),
           y = c(1, 0.8),
           size = 7,
           hjust = 0) +
  labs(x = "surprise & importance",
       y = y) +
  scale_x_continuous(breaks = seq(-1, 1, 0.5)) +
  scale_y_continuous(breaks = seq(-1, 1, 0.5)) +
  coord_cartesian(xlim = c(-1.25, 1.25),
                  ylim = c(-1.25, 1.25)) +
  theme(text = element_text(size = 24))

# ggsave("../../figures/plots/experiment4_wrongfulness_surprise_importance_scaled.pdf",
#        width = 5,
#        height = 5)
```

### Individual participant analysis 

```{r fig.height=12, fig.width=8}
set.seed(1)

df.plot = df.exp4.individual.correlations %>% 
  pivot_longer(cols = -participant,
               names_to = "index",
               values_to = "correlation") %>% 
  mutate(index = str_replace(index, "r_", "r("),
         index = str_replace(index, "_", ", "),
         index = str_c(index, ")"),
         index = factor(index, levels = 
                          c("r(pivotality, importance)",
                            "r(badness, surprise)",
                            "r(surprise, responsibility)",
                            "r(importance, responsibility)",
                            "r(surprise, wrongfulness)",
                            "r(importance, wrongfulness)"))) %>% 
  drop_na()

df.plot1 = df.plot %>% 
  group_by(index) %>% 
  summarize(value = smean.cl.boot(correlation)) %>% 
  mutate(estimate = c("mean", "low", "high")) %>% 
  pivot_wider(names_from = estimate,
              values_from = value)

ggplot(data = df.plot) + 
  geom_vline(xintercept = seq(-1, 1, 0.25),
             linetype = 2,
             alpha = 0.1) + 
  geom_density(mapping = aes(x = correlation),
               fill = "lightblue",
               bw = 0.05) +
  geom_pointrange(data = df.plot1,
                  mapping = aes(x = mean,
                                xmin = low,
                                xmax = high,
                                y = 0.3),
                  shape = 21, 
                  fill = "white",
                  size = 1) + 
  facet_wrap(~index, ncol = 1) + 
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  theme(text = element_text(size = 16))
```


## Misc


### Votes

```{r}
set.seed(1)

df.plot = df.exp4.long %>% 
  mutate(votes = as.factor(votes),
         question = factor(question,
                           levels = c("importance", "responsibility",
                                      "surprise", "badness", "wrongfulness")))

ggplot(data = df.plot,
       mapping = aes(x = votes,
                     y = rating,
                     color = question)) + 
  stat_summary(fun.data = "mean_cl_boot",
               position = position_dodge(width = 0.5))
  
```


### Votes, badness, responsibility 

```{r}
y = "responsibility"

df.plot = df.exp4.long %>% 
  filter(question == y) %>% 
  mutate(across(.cols = c(bad, votes, trial),
                .fns = ~ as.factor(.)))

ggplot(data = df.plot, 
       mapping = aes(x = votes,
                     y = rating,
                     color = bad)) + 
  stat_summary(fun.data = "mean_cl_boot",
               position = position_dodge(width = 0.9)) + 
  geom_point(alpha = 0.05,
             position = position_jitterdodge(dodge.width = 0.9,
                                             jitter.width = 0.1,
                                             jitter.height = 0)) +
  # scale_color_brewer(palette = "Set1") + 
  theme(legend.position = "bottom") + 
  guides(color = guide_legend(nrow = 1))
```

```{r}
df.exp4.regression.means %>% 
  select(trial, bad, votes, responsibility) %>% 
  left_join(df.exp4.trialinfo,
            by = c("trial", "bad", "votes"))

```

### Per trial 

```{r fig.height=20, fig.width=8}
df.plot = df.exp4.long %>% 
  mutate(across(.cols = c(bad, votes, trial),
                .fns = ~ as.factor(.)))

ggplot(data = df.plot,
       mapping = aes(x = trial,
                     y = rating,
                     group = question,
                     shape = votes,
                     color = bad)) + 
  stat_summary(fun.data = "mean_cl_boot",
               position = position_dodge(width = 1)) + 
  geom_point(alpha = 0.1,
             position = position_dodge(width = 1)) + 
  facet_wrap(~ question, ncol = 1)
```

## Session info 

Information about this R session including which version of R was used, and what packages were loaded. 

```{r}
sessionInfo()
```
